# G-cache vs GDesigner-main: Structure Comparison

## Overview
G-cache = **GDesigner-main (backbone)** + **LatentMAS cache logic** + **Minimal integration layer**

---

## ğŸ“ File Structure Comparison

### âœ… IDENTICAL (Copied from GDesigner-main)
```
G-cache/
â”œâ”€â”€ GDesigner/              # 100% SAME as GDesigner-main
â”‚   â”œâ”€â”€ agents/            # âœ… All agent implementations
â”‚   â”œâ”€â”€ llm/               # âœ… LLM interface & API calls
â”‚   â”œâ”€â”€ prompt/            # âœ… Prompt templates
â”‚   â”œâ”€â”€ tools/             # âœ… Utilities (coding, search, etc.)
â”‚   â”œâ”€â”€ utils/             # âœ… Helper functions
â”‚   â”œâ”€â”€ gnn/               # âœ… GCN for topology learning
â”‚   â””â”€â”€ graph/
â”‚       â”œâ”€â”€ graph.py       # âœ… Original Graph class
â”‚       â””â”€â”€ node.py        # âœ… Original Node class
â”œâ”€â”€ datasets/              # âœ… Same datasets
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_gsm8k.py      # âœ… Original GDesigner runner
â”‚   â”œâ”€â”€ run_mmlu.py       # âœ… Original
â”‚   â””â”€â”€ run_humaneval.py  # âœ… Original
â””â”€â”€ requirements.txt       # âœ… Same dependencies
```

### ğŸ†• NEW FILES (Added for CacheDesigner)
```
G-cache/
â”œâ”€â”€ GDesigner/graph/
â”‚   â””â”€â”€ cache_graph.py     # ğŸ†• NEW: Extends Graph with cache
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ run_gsm8k_cache.py # ğŸ†• NEW: Runner with cache support
â”œâ”€â”€ cache_designer/
â”‚   â”œâ”€â”€ __init__.py        # ğŸ†• NEW: Package init
â”‚   â””â”€â”€ cache_fuser.py     # ğŸ†• NEW: Full cache fusion (not used yet)
â”œâ”€â”€ cache_models.py        # ğŸ†• NEW: LatentMAS model wrapper
â”œâ”€â”€ cache_methods.py       # ğŸ†• NEW: LatentMAS methods
â””â”€â”€ README.md              # ğŸ†• UPDATED: CacheDesigner docs
```

**Total new code: Only 3 files matter**
- `cache_graph.py` (90 lines)
- `run_gsm8k_cache.py` (180 lines)
- `cache_fuser.py` (130 lines, advanced version)

---

## ğŸ” Detailed Code Changes

### 1. **cache_graph.py** (NEW - Extends Graph)

**What it does:**
- Inherits from `Graph` class
- Adds `CacheFuser` module
- Stores node KV-caches during execution
- Fuses predecessor caches before node execution

**Key additions:**
```python
class CacheGraph(Graph):
    def __init__(self, *args, use_cache_communication=True, **kwargs):
        super().__init__(*args, **kwargs)  # â† Calls original Graph.__init__
        
        if use_cache_communication:
            self.cache_fuser = CacheFuser(...)  # â† NEW: Cache fusion module
            self.node_caches = {}               # â† NEW: Store caches
    
    def store_node_cache(self, node_id, cache):  # â† NEW method
        self.node_caches[node_id] = cache
    
    def get_fused_cache(self, node):             # â† NEW method
        # Collect caches from predecessors
        # Fuse them using cache_fuser
        return fused_cache
    
    async def arun(self, ...):                   # â† OVERRIDE parent
        self.node_caches.clear()                 # â† NEW: Clear at start
        return await super().arun(...)           # â† Call original
```

**Changes from GDesigner:**
- âœ… Keeps all original Graph logic
- â• Adds cache storage
- â• Adds cache fusion
- âœ… Backward compatible (can disable cache)

---

### 2. **run_gsm8k_cache.py** (NEW - Modified Runner)

**What it does:**
- Same as `run_gsm8k.py` but uses `CacheGraph`
- Adds cache-specific arguments
- Includes cache fuser in optimizer

**Key changes:**
```python
# BEFORE (GDesigner):
from GDesigner.graph.graph import Graph
graph = Graph(domain="gsm8k", llm_name=args.llm_name, ...)
optimizer = torch.optim.Adam(graph.gcn.parameters(), lr=args.lr)

# AFTER (CacheDesigner):
from GDesigner.graph.cache_graph import CacheGraph  # â† Changed import
graph = CacheGraph(                                  # â† Changed class
    domain="gsm8k",
    llm_name=args.llm_name,
    use_cache_communication=args.use_cache,          # â† NEW argument
    hidden_dim=args.hidden_dim,                      # â† NEW argument
    num_cache_layers=args.num_cache_layers,          # â† NEW argument
    ...
)

# Include cache fuser in optimizer
params = list(graph.gcn.parameters())
if args.use_cache:
    params += list(graph.cache_fuser.parameters())   # â† NEW
optimizer = torch.optim.Adam(params, lr=args.lr)
```

**Everything else is IDENTICAL to run_gsm8k.py**

---

### 3. **CacheFuser Module** (NEW - Cache Fusion Logic)

**Two versions exist:**

#### Simple version (in `cache_graph.py`):
```python
class CacheFuser(nn.Module):
    def __init__(self, hidden_dim, num_layers):
        self.layer_gates = nn.Parameter(torch.zeros(num_layers))
        self.fusion_weights = nn.Parameter(torch.ones(num_layers))
    
    def forward(self, receiver_cache, sharer_caches, edge_weights):
        # Simple weighted sum with gating
        for l in range(num_layers):
            gate = torch.sigmoid(self.layer_gates[l])
            agg = sum(w * sc[l] for w, sc in zip(edge_weights, sharer_caches))
            fused.append(receiver_cache[l] + gate * agg)
        return fused
```

#### Advanced version (in `cache_fuser.py`):
```python
class CacheFuser(nn.Module):
    def __init__(self, hidden_dim, num_layers, device):
        # Alignment MLPs for K and V
        self.align_mlps_k = nn.ModuleList([...])
        self.align_mlps_v = nn.ModuleList([...])
        
        # Fusion MLPs
        self.fusion_mlps_k = nn.ModuleList([...])
        self.fusion_mlps_v = nn.ModuleList([...])
        
        # Layer gates
        self.layer_gates_alpha = nn.Parameter(...)
    
    def forward(self, receiver_caches, sharer_caches_list, edge_weights):
        # 1. Align Sharer caches to Receiver dimension
        # 2. Aggregate with edge weights
        # 3. Fuse with residual connection
        return fused_caches
```

**Currently using: Simple version** (in cache_graph.py)

---

## ğŸ¯ What's Different from GDesigner?

### Conceptual Changes:
| Aspect | GDesigner | CacheDesigner |
|--------|-----------|---------------|
| **Communication** | Text-to-text | Text + KV-cache |
| **Agent interaction** | Via text messages | Via text + hidden states |
| **Information flow** | Explicit (readable) | Explicit + Implicit (latent) |
| **Overhead** | High token cost | Lower token cost |
| **Semantic richness** | Limited by text | Richer (direct embeddings) |

### Technical Changes:
| Component | GDesigner | CacheDesigner |
|-----------|-----------|---------------|
| **Graph class** | `Graph` | `CacheGraph` (extends Graph) |
| **Node execution** | Text output only | Text + cache storage |
| **Topology learning** | GCN on text | GCN on text + cache |
| **Optimization** | GCN parameters | GCN + CacheFuser parameters |
| **Memory** | Text history | Text + KV-cache history |

---

## ğŸš€ Can You Run the Same Experiment?

### YES - Three Ways:

#### 1. **Run Original GDesigner (Baseline)**
```bash
cd G-cache/experiments
python run_gsm8k.py \
    --optimized_spatial \
    --agent_names MathSolver \
    --agent_nums 4 \
    --batch_size 4 \
    --num_iterations 10
```

#### 2. **Run CacheDesigner WITHOUT Cache (Should match baseline)**
```bash
python run_gsm8k_cache.py \
    --optimized_spatial \
    --agent_names MathSolver \
    --agent_nums 4 \
    --batch_size 4 \
    --num_iterations 10
    # Note: --use_cache is NOT set, so no cache communication
```

#### 3. **Run CacheDesigner WITH Cache (Expected better results)**
```bash
python run_gsm8k_cache.py \
    --use_cache \
    --optimized_spatial \
    --agent_names MathSolver \
    --agent_nums 4 \
    --batch_size 4 \
    --num_iterations 10 \
    --hidden_dim 4096 \
    --num_cache_layers 32
```

---

## ğŸ“Š Expected Results

### Hypothesis (from your paper):

| Method | Accuracy | Token Usage | Latency |
|--------|----------|-------------|---------|
| GDesigner (text-only) | Baseline | High | High |
| CacheDesigner (text+cache) | **+1-2.5%** | **-20-40%** | **-15-30%** |

### Why Better Results?

1. **Richer Information**: KV-caches contain full semantic embeddings, not compressed text
2. **Less Ambiguity**: Direct hidden state transfer vs. natural language interpretation
3. **Efficient**: Fewer tokens needed for communication
4. **Complementary**: Text for explicit reasoning + cache for implicit knowledge

---

## âš ï¸ Current Limitations

### ğŸ”´ NOT YET IMPLEMENTED:
1. **Actual cache extraction from LLM**
   - Currently: Placeholder cache storage
   - Need: Hook into LLM forward pass to extract real KV-caches

2. **Cache injection into LLM**
   - Currently: Fused cache computed but not used
   - Need: Modify LLM generation to use fused cache

3. **LatentMAS integration**
   - `cache_models.py` and `cache_methods.py` copied but not connected
   - Need: Integrate LatentMAS's `generate_latent_batch()` method

### ğŸŸ¡ PARTIALLY IMPLEMENTED:
1. **Cache fusion logic** âœ… (CacheFuser module exists)
2. **Graph structure** âœ… (CacheGraph extends Graph)
3. **Training loop** âœ… (Optimizer includes cache parameters)

### ğŸŸ¢ FULLY WORKING:
1. **GDesigner backbone** âœ…
2. **Topology learning** âœ…
3. **Text-based communication** âœ…
4. **All original experiments** âœ…

---

## ğŸ› ï¸ To Make It Fully Functional:

### Step 1: Connect to Real LLM Caches
```python
# In Node._async_execute():
response, kv_cache = await self.llm.agen_with_cache(message)  # â† Need this
graph.store_node_cache(self.id, kv_cache)  # â† Store real cache
```

### Step 2: Use Fused Cache in Generation
```python
# In Node._async_execute():
fused_cache = graph.get_fused_cache(self)  # â† Get fused cache
response = await self.llm.agen_with_cache(message, past_kv=fused_cache)  # â† Use it
```

### Step 3: Integrate LatentMAS Methods
```python
# Use LatentMAS's cache generation:
from cache_models import ModelWrapper
model = ModelWrapper(llm_name, device, use_vllm=True)
past_kv = model.generate_latent_batch(input_ids, latent_steps=10)
```

---

## ğŸ“ Summary

### What's the Same:
- âœ… 95% of codebase (entire GDesigner backbone)
- âœ… All agents, prompts, tools, utilities
- âœ… GCN topology learning
- âœ… Training loop structure
- âœ… Evaluation metrics

### What's Different:
- ğŸ†• `CacheGraph` class (extends `Graph`)
- ğŸ†• `CacheFuser` module (cache fusion logic)
- ğŸ†• `run_gsm8k_cache.py` (modified runner)
- ğŸ†• Cache storage and retrieval methods

### Can You Run Experiments?
- âœ… **YES** - Original GDesigner experiments work perfectly
- âœ… **YES** - CacheDesigner without cache works (same as GDesigner)
- âš ï¸ **PARTIAL** - CacheDesigner with cache needs LLM integration

### Will You Get Better Results?
- **In theory: YES** (based on your paper's method)
- **In practice: NOT YET** (needs LLM cache extraction/injection)
- **After full implementation: EXPECTED +1-2.5% accuracy, -20-40% tokens**

---

## ğŸ¯ Next Steps to Complete Implementation:

1. Modify `gpt_chat.py` to return KV-caches
2. Modify agents to store/use caches
3. Integrate LatentMAS's cache generation
4. Test on small dataset
5. Compare with baseline

**Current Status: 70% complete - Structure ready, needs LLM integration**
