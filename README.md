# G-cache: Graph-Guided KV-Cache Communication for Multi-Agent LLMs

Combines **GDesigner's** graph topology with **LatentMAS's** KV-cache generation.

---

# conda activate /local3/ericjiang/envs/lean_agent

## ğŸš€ Quick Start

```bash
cd experiments

# Hybrid mode (RECOMMENDED) - small GPU + free API
python run_gsm8k_cache_API.py --llm_name hybrid_cache_v2 --use_cache --generation_mode hybrid --latent_only --add_role
```

---

## ğŸ“‹ Run Scripts for All Tasks

### Math Reasoning Tasks

**GSM8K (Grade School Math)**
```bash
cd experiments
python run_gsm8k_cache_API.py --llm_name hybrid_cache_v2 --use_cache --generation_mode hybrid --latent_only --latent_steps 10
```

### Science & Medical Tasks

**GPQA (Graduate-Level Science)**
```bash
cd experiments
python run_gpqa_cache_API.py --llm_name hybrid_cache_v2 --use_cache --generation_mode hybrid --latent_only --latent_steps 10
```

**MedQA (Medical Q&A)**
```bash
cd experiments
python run_medqa_cache_API.py --llm_name hybrid_cache_v2 --use_cache --generation_mode hybrid --latent_only --latent_steps 10
```

### Code Generation Tasks

**HumanEval (Function-Level Code)**
```bash
cd experiments
python run_humaneval_cache_API.py --llm_name hybrid_cache_v2 --use_cache --generation_mode hybrid --latent_only --latent_steps 10
```

**MBPP+ (Basic Python Problems)**
```bash
cd experiments
python run_mbppplus_cache_API.py --llm_name hybrid_cache_v2 --use_cache --generation_mode hybrid --latent_only --latent_steps 10
```

### Common Options

```bash
# Run specific question
--question_id 0  # Run only question 0

# Adjust batch size
--batch_size 1  # Process 1 question at a time

# Number of iterations
--num_iterations 10  # Run 10 batches

# Latent steps (reasoning depth)
--latent_steps 10  # 10 latent reasoning steps per agent

# Generation modes
--generation_mode hybrid     # Local model + API (BEST quality)
--generation_mode api_hint   # API only with cache hint
--generation_mode local      # Local model only (no API)

# Agent configuration
--agent_nums 4  # Use 4 agents
```

### Agent Types

By default, all scripts use **diverse agents** (Math, Analyst, Code, Inspector).

To switch to **uniform agents** (all Math Solver), edit the script and uncomment:
```python
# Option 2: All same agents (uncomment to use)
agent_names = ['MathSolverCacheV2'] * sum(args.agent_nums)
```

---

## ğŸ“ What Gets Updated During Training?

### Critical Insight: LatentMAS vs G-cache

**LatentMAS (original):**
- âŒ **NO training** - Inference only!
- âŒ No optimizer, no loss.backward()
- âŒ All models frozen
- Just runs multi-agent inference with cache

**G-cache (this project):**
- âœ… **HAS training** - Learns graph structure!
- âœ… Optimizer updates GCN + CacheFuser
- âœ… Learns from task performance

### Models in G-cache

| Model | Trainable? | Updated? | Purpose | LatentMAS Has This? |
|-------|-----------|----------|---------|---------------------|
| **1. GCN** | âœ… Yes | âœ… Yes | Graph edge weights | âŒ No (NEW in G-cache) |
| **2. CacheFuser** | âœ… Yes | âœ… Yes | Cache fusion weights | âŒ No (NEW in G-cache) |
| **3. Small Local Model** | âŒ Frozen | âŒ No | Cache generation | âœ… Yes (frozen in both) |
| **4. API Model** | âŒ External | âŒ No | Text generation | âœ… Yes (frozen in both) |

### Training Code (G-cache ONLY)

```python
# File: experiments/run_gsm8k_cache_API.py

# Step 1: Setup optimizer with trainable components (NEW in G-cache)
params = list(graph.gcn.parameters())  # â† GCN weights (NEW)
if args.use_cache:
    params += list(graph.cache_fuser.parameters())  # â† CacheFuser weights (NEW)
optimizer = torch.optim.Adam(params, lr=0.1)

# Step 2: Forward pass
answer, log_prob = await graph.arun(question)

# Step 3: Compute loss (NEW in G-cache)
is_correct = (answer == gold_answer)
utility = float(is_correct)
loss = -log_prob * utility

# Step 4: Backprop (NEW in G-cache)
optimizer.zero_grad()
loss.backward()  # Updates:
                 # âœ… GCN.edge_weights (NEW)
                 # âœ… CacheFuser.layer_gates (NEW)
                 # âœ… CacheFuser.fusion_weights (NEW)
                 # âŒ Small local model (frozen - same as LatentMAS)
                 # âŒ API model (external - same as LatentMAS)
optimizer.step()
```

### What LatentMAS Does (Inference Only)

```python
# File: LatentMAS/run.py

# NO optimizer!
# NO loss!
# NO training!

# Just inference:
for item in dataset:
    result = method.run_batch([item])  # Generate answer
    is_correct = (result['prediction'] == item['gold'])
    # That's it - no backprop!
```

### Comparison

| Aspect | LatentMAS | G-cache |
|--------|-----------|---------|
| **Training** | âŒ No | âœ… Yes |
| **Optimizer** | âŒ No | âœ… Yes (Adam) |
| **Loss** | âŒ No | âœ… Yes (-log_prob * utility) |
| **Backprop** | âŒ No | âœ… Yes |
| **Trainable params** | 0 | GCN + CacheFuser |
| **Cache generation** | âœ… Yes (frozen model) | âœ… Yes (frozen model) |
| **Graph structure** | Fixed (sequential/hierarchical) | âœ… Learned |
| **Cache fusion** | Fixed (concatenation) | âœ… Learned |

### Guarantee: No Missing Updates

**LatentMAS updates**: 0 models (inference only)

**G-cache updates**: 2 models
1. âœ… GCN - Added to optimizer (line 108)
2. âœ… CacheFuser - Added to optimizer (line 110)

**Verification:**
```python
# Check what's in optimizer
params = list(graph.gcn.parameters())  # â† All GCN params
params += list(graph.cache_fuser.parameters())  # â† All CacheFuser params
optimizer = torch.optim.Adam(params, lr=0.1)

# These are ALL trainable parameters in the system!
# Small model and API are explicitly frozen/external
```

**Guarantee**: âœ… We update everything that should be updated!

---

## ğŸ”— How Graph Functions Connect with Cache Functions

### The Connection Chain

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. CacheGraph (cache_graph.py)                                 â”‚
â”‚    - Manages cache storage: node_caches = {}                   â”‚
â”‚    - Provides: get_fused_cache(), store_node_cache()           â”‚
â”‚    - Trainable: CacheFuser âœ…                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ passes graph reference
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. MathSolverCache (math_solver_cache.py)                      â”‚
â”‚    - Receives: self.graph = graph                              â”‚
â”‚    - Calls: graph.get_fused_cache(self)                        â”‚
â”‚    - Calls: graph.store_node_cache(self.id, cache)             â”‚
â”‚    - Trainable: None                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ calls LLM
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. HybridCacheLLM (llm_cache_hybrid.py)                        â”‚
â”‚    - Receives: past_key_values from graph                      â”‚
â”‚    - Calls: hybrid_model.generate_latent_batch()               â”‚
â”‚    - Returns: (text, kv_cache) to agent                        â”‚
â”‚    - Trainable: None                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ calls LatentMAS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. HybridCacheModel (hybrid_cache_model.py)                    â”‚
â”‚    - EXACT LatentMAS implementation                            â”‚
â”‚    - generate_latent_batch(past_key_values=fused_cache)        â”‚
â”‚    - Returns: new_cache                                        â”‚
â”‚    - Trainable: None (frozen) âŒ                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Connections Summary

| Connection | From | To | Data | Function |
|------------|------|----|----|----------|
| 1 | CacheGraph | Agent | `graph` reference | `node.graph = self` |
| 2 | Agent | CacheGraph | `self` | `graph.get_fused_cache(self)` |
| 3 | CacheGraph | Agent | `fused_cache` | Returns fused cache |
| 4 | Agent | LLM | `past_key_values` | `llm.agen_with_cache(..., past_key_values)` |
| 5 | LLM | LatentMAS | `past_key_values` | `generate_latent_batch(..., past_key_values)` |
| 6 | LatentMAS | LLM | `new_cache` | Returns cache |
| 7 | LLM | Agent | `(text, cache)` | Returns tuple |
| 8 | Agent | CacheGraph | `cache` | `graph.store_node_cache(id, cache)` |

---

## ğŸ¯ Four Modes

### Mode 1: Pure Local with Real Cache (BEST for Research) âœ…

```bash
cd experiments
export CUDA_VISIBLE_DEVICES=1,2  # Use 2 GPUs for 7B model
python run_gsm8k_cache.py --llm_name Qwen/Qwen3-8B --use_cache --device cuda
```

**What it does**:
- âœ… **Real KV-cache** from local model (LatentMAS)
- âœ… **Alignment matrix** enabled by default
- âœ… Trains GCN + CacheFuser
- âœ… No API needed (fully local)
- âœ… Uses HuggingFace transformers (no vLLM)

**GPU**: ~16GB for 8B model (or ~4GB for 1.7B model)

**Model options**:
- `Qwen/Qwen3-1.7B` - Small, ~4GB GPU
- `Qwen/Qwen3-4B` - Medium, ~8GB GPU  
- `Qwen/Qwen3-8B` - Large, ~16GB GPU (recommended)

**Use this for**: Real cache experiments, full control

---

### Mode 2: Hybrid (Small GPU + API) âœ…

```bash
cd experiments
export CUDA_VISIBLE_DEVICES=1  # Only need 1 GPU
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache
```

**What it does**:
- âœ… Small local model (1.5B) generates **real KV-cache**
- âœ… API (qwen-flash) generates final text (faster & cheaper than qwen-plus)
- âœ… **Alignment matrix** enabled
- âš ï¸ Requires API key

**GPU**: ~4GB only

**API models available**: `qwen-flash` (default), `qwen-plus`, `qwen-turbo`

**Use this for**: When you have API access

---

### Mode 3: API Baseline (No Cache)

```bash
cd experiments
python run_gsm8k_cache_API.py --llm_name qwen-flash  # or qwen-plus, qwen-turbo
```

**What it does**:
- âŒ No real cache (text-based only)
- Uses API for everything
- Baseline for comparison

**GPU**: 0GB

---

### Mode 4: Local Cache (Deprecated)

```bash
cd experiments
export CUDA_VISIBLE_DEVICES=1,2
python run_gsm8k_cache_API.py --llm_name local_cache --use_cache
```

**Note**: Use Mode 1 (`run_gsm8k_cache.py`) instead - it's the same but better tested

---

## ğŸ”§ Setup

```bash
# 1. Install dependencies
pip install torch transformers openai python-dotenv

# 2. Set API key in .env file (already done!)
# Just edit .env:
# API_KEY=your_dashscope_api_key

# 3. Run (no export needed!)
cd experiments
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache
```

**Note**: The code automatically loads `API_KEY` from `.env` - no need to export anything!

---

## ğŸ”¬ LatentMAS Alignment Matrix (Training-Free)

### What is it?

LatentMAS uses a **projection matrix** `W_a` to align hidden states back to valid input embeddings:

```
e = h * W_a, where W_a â‰ˆ W_out^(-1) * W_in
```

**Problem**: Hidden states from last layer have different distribution than input embeddings

**Solution**: Linear transformation that maps output space â†’ input space (training-free!)

### Implementation in LatentMAS

**File**: `LatentMAS/models.py`

```python
class ModelWrapper:
    def _build_latent_realign_matrix(self, model, device, args):
        """Compute W_a = (W_out^T * W_out)^(-1) * W_out^T * W_in"""
        input_weight = model.get_input_embeddings().weight   # W_in
        output_weight = model.get_output_embeddings().weight # W_out
        
        # Solve: W_out * W_a = W_in
        gram = torch.matmul(output_weight.T, output_weight)  # W_out^T * W_out
        reg = 1e-5 * torch.eye(gram.shape[0])                # Regularization
        gram = gram + reg
        rhs = torch.matmul(output_weight.T, input_weight)    # W_out^T * W_in
        realign_matrix = torch.linalg.solve(gram, rhs)       # W_a
        
        target_norm = input_weight.norm(dim=1).mean()        # For normalization
        return realign_matrix, target_norm
    
    def _apply_latent_realignment(self, hidden, model):
        """Apply: e = normalize(h * W_a)"""
        matrix, target_norm = self._ensure_latent_realign_matrix(model, hidden.device)
        aligned = torch.matmul(hidden.float(), matrix)       # h * W_a
        
        # Normalize to match input embedding norms
        aligned_norm = aligned.norm(dim=-1, keepdim=True).clamp_min(1e-6)
        aligned = aligned * (target_norm / aligned_norm)
        return aligned.to(hidden.dtype)
    
    def generate_latent_batch(self, ...):
        for step in range(latent_steps):
            # KEY: Apply alignment before feeding back
            latent_vec = self._apply_latent_realignment(last_hidden, self.model)
            latent_embed = latent_vec.unsqueeze(1)
            
            outputs = self.model(
                inputs_embeds=latent_embed,  # â† Aligned embedding
                ...
            )
```

**Usage**: `python run.py --latent_space_realign` (optional flag)

### Does G-cache Have This?

**Answer**: âœ… **YES! Now implemented!**

**File**: `G-cache/hybrid_cache_model.py`

```python
class HybridCacheModel:
    def __init__(self, ..., use_alignment: bool = True):
        # Build alignment matrix (training-free, computed once)
        if self.use_alignment:
            self._build_alignment_matrix()
    
    def _build_alignment_matrix(self):
        """Build W_a â‰ˆ W_out^(-1) * W_in"""
        W_in = self.cache_model.get_input_embeddings().weight
        W_out = self.cache_model.get_output_embeddings().weight
        gram = torch.matmul(W_out.T, W_out) + 1e-5 * I
        rhs = torch.matmul(W_out.T, W_in)
        self._alignment_matrix = torch.linalg.solve(gram, rhs)
    
    def _apply_alignment(self, hidden):
        """Apply: e = normalize(h * W_a)"""
        aligned = torch.matmul(hidden, self._alignment_matrix)
        return aligned * (target_norm / ||aligned||)
    
    def generate_latent_batch(self, ...):
        for step in range(latent_steps):
            latent_vec = self._apply_alignment(last_hidden)  # âœ… Now aligned!
            latent_embed = latent_vec.unsqueeze(1)
```

**Usage**: Enabled by default! Disable with `use_alignment=False`

### Comparison

| Aspect | LatentMAS | G-cache |
|--------|-----------|---------||
| **Alignment matrix** | âœ… Optional (`--latent_space_realign`) | âœ… **Enabled by default!** |
| **Matrix computation** | `W_a â‰ˆ W_out^(-1) * W_in` | âœ… Same |
| **Normalization** | âœ… Match input embedding norms | âœ… Same |
| **Training-free** | âœ… Yes (computed once) | âœ… Yes (computed once) |
| **Implementation** | 3 methods in ModelWrapper | âœ… 2 methods in HybridCacheModel |

### Benefits in G-cache

**âœ… Enabled by default**:
- Better cache quality (aligned embeddings)
- Training-free (computed once at init)
- Minimal overhead (one matrix multiply per latent step)

**Disable if needed**:
```python
model = HybridCacheModel(use_alignment=False)  # Disable alignment
```

---

## ğŸ¯ Summary

**What**: Graph manages cache flow, LatentMAS generates cache

**LatentMAS**: Inference only (no training) + optional alignment matrix

**G-cache**: Trains GCN + CacheFuser (learns graph structure + fusion)

**Frozen**: Small local model + API model (same as LatentMAS)

**Alignment**: Both have it! (LatentMAS optional, G-cache default)

**Run (Pure Local - RECOMMENDED)**:
```bash
cd experiments
export CUDA_VISIBLE_DEVICES=1,2
python run_gsm8k_cache.py --llm_name Qwen/Qwen3-1.7B --use_cache --device cuda
```

**That's it!** ğŸ‰

---

## ğŸ”„ Cache Flow Explained

### What Are Agent IDs?

Agent IDs like `3KsM`, `QXan`, `5Hp5` are randomly generated 4-character identifiers for each agent node in the graph.

### Agent Graph Structure

```
     3KsM (MathSolver) â†’ cache: 2377 tokens
     QXan (MathSolver) â†’ cache: 474 tokens  
     5Hp5 (MathSolver) â†’ cache: 181 tokens
       â†“ â†“ â†“ (all feed into)
     5xcm (Decision Maker)
       â†“ receives: 3KsM's cache (first one)
       â†“ generates: final answer
```

### What Comes From Cache vs LLM?

| Component | What It Does | Visible? | Example |
|-----------|-------------|----------|----------|
| **Cache (GPU)** | Stores reasoning as tensors | âŒ No | `torch.Size([1, 2, 2377, 128])` |
| **API (qwen-flash)** | Generates text responses | âœ… Yes | "Let's solve step by step..." |
| **Cache Fusion** | Combines multiple caches | âŒ No | Internal process |
| **Graph Structure** | Routes cache between agents | âœ… Yes | Shown in logs |

**Key Insight**: Cache = "compressed thoughts" that agents pass to each other. API converts these thoughts into readable text.

### âš ï¸ Important: How Cache is Used

**Cache is NOT directly used by API!** Here's the actual flow:

1. **Small local model (GPU)** generates KV-cache tensors
2. **Cache is converted to text context**: `"[Using 28 layers of cached reasoning from previous agents, 2377 tokens]"`
3. **This text is prepended to the API prompt**
4. **API (qwen-flash) generates response** based on the text context (NOT the raw cache)

**Why?** APIs don't support KV-cache input - they only accept text. The cache serves as a "hint" to the API about what previous agents computed.

### Cache Flow Example

**Without Cache (Cold Start)**:
```
Agent 3KsM:
  ğŸ†• No input cache
  ğŸ§  Generates from scratch
  ğŸ“ Output: "Let's solve... Total = $64"
  ğŸ’¾ Stores cache: 2377 tokens
```

**With Cache (Warm Start)**:
```
Agent 5xcm:
  ğŸ”— Receives cache from 3KsM (2377 tokens)
  ğŸ§  Continues from 3KsM's reasoning
  ğŸ“ Output: "After reviewing... The answer is 64"
  ğŸ’¾ Stores cache: 3724 tokens (grew from 2377)
```

### Sequence Length Mismatch

**Problem**: Can't fuse caches of different lengths (2377 â‰  474 â‰  181)

**Solution (LatentMAS-proven)**: Use first cache only
- Takes 3KsM's cache (2377 tokens)
- Ignores QXan and 5Hp5 caches
- Sequential passing (proven by LatentMAS paper)

### Debug Markers

Look for these in output:
```
ğŸ”— [CACHE] Using input cache with 28 layers     â† Cache is being used
ğŸ†• [CACHE] No input cache - generating from scratch  â† No cache (first agent)
âœ… [CACHE] Generated cache with 28 layers, seq_len=2377  â† Cache created
ğŸ¤– [API] Calling qwen-flash to generate text...  â† API generates text
ğŸ“ [API] Generated 1234 characters of text       â† Text output
```

### Cache Structure Details

**What is `past_key_values`?**

Type: Tuple of tuples (NOT text, NOT string)

```python
past_key_values = (
    (key_tensor_0, value_tensor_0),    # Layer 0: [1, 2, 2377, 128]
    (key_tensor_1, value_tensor_1),    # Layer 1: [1, 2, 2377, 128]
    # ... 26 more layers ...
    (key_tensor_27, value_tensor_27)   # Layer 27: [1, 2, 2377, 128]
)
```

**Total**: 28 layers Ã— 2 tensors (key + value) = 56 tensors

**Tensor shape**: `[batch=1, heads=2, seq_len=2377, hidden_dim=128]`
- Data type: `torch.bfloat16` (16-bit floating point)
- Device: `cuda:0` (GPU memory)

**Why Can't API Use This?**

API endpoints only accept TEXT, not binary tensors.

**Solution**: Convert to text hint
```python
# Cache (tensor): 56 tensors Ã— millions of numbers
past_key_values = ((tensor1, tensor2), ...)

# Converted to text:
cache_info = "[Using 28 layers of cached reasoning, 2377 tokens]"

# Sent to API:
prompt = cache_info + "\n\n" + original_prompt
```

**Analogy**: Cache = full book (tensor data), API receives = "This person read a 300-page book" (text hint)

### Two Ways to Generate Text

**Method 1: Local Model with Cache Tensors (REAL cache usage)**
```python
# Function: generate_text_batch()
# Uses: Local GPU model (Qwen2.5-1.5B)
# Cache: Passed DIRECTLY as tensors to model.generate()

outputs = model.generate(
    input_ids=input_ids,
    past_key_values=cache_tensors,  # â† REAL cache usage!
    ...
)
```

**Method 2: API with Text Hint (Simulated cache)**
```python
# Function: generate_text_batch_api()
# Uses: API (qwen-flash)
# Cache: Converted to text hint, prepended to prompt

prompt = "[Using 28 layers of cache, 2377 tokens]\n\n" + original_prompt
response = api.chat.completions.create(
    messages=[{"content": prompt}],  # â† Text hint only
    ...
)
```

**Comparison**:

| Aspect | Local (Real Cache) | API (Text Hint) |
|--------|-------------------|------------------|
| **Cache Format** | Tensor objects | Text string |
| **Cache Usage** | Direct (model.generate) | Indirect (text context) |
| **GPU Required** | Yes (~4GB) | No |
| **Speed** | Faster (cache reuse) | Slower (no cache reuse) |
| **Quality** | Good (small model) | Better (large API model) |
| **Cost** | GPU cost | API cost |
| **Best For** | Pure local | API only | **Production** â­ |

**Key Difference**:
- **Local**: `model.generate(past_key_values=cache_tensors)` â† Tensors used directly
- **API**: `api.create(messages=[text_hint])` â† Only text hint sent

**Why API Can't Use Cache Tensors**:
- API endpoints only accept JSON/text
- Can't send binary tensor data over HTTP
- Solution: Convert cache metadata to text hint

**Method 3: TRUE Hybrid (BEST - combines both!)**
```python
# Function: generate_text_batch_hybrid()
# Step 1: Local model uses cache tensors (fast)
# Step 2: API refines output (high quality)

local_text, new_cache = model.generate_text_batch(
    input_ids, past_key_values=cache_tensors  # â† Real cache!
)
api_text, _ = await api.generate_text_batch_api(
    messages_with_local_context  # â† API refines
)
return api_text, new_cache  # Best of both!
```

**Trade-off**:
- Local only: Real cache âœ… but small model âŒ
- API only: Large model âœ… but no cache âŒ
- **Hybrid**: Real cache âœ… + Large model âœ… = BEST â­

---

## ğŸ”Œ API Model Options

### Default: qwen-flash âœ…

The code now uses **qwen-flash** by default (faster and cheaper than qwen-plus).

### Available Models

| Model | Speed | Cost | Use Case |
|-------|-------|------|----------|
| **qwen-flash** | âš¡ Fastest | ğŸ’° Cheapest | Default (recommended) |
| qwen-plus | ğŸ¢ Medium | ğŸ’°ğŸ’° Medium | More capable |
| qwen-turbo | âš¡ Fast | ğŸ’° Cheap | Alternative |

### Usage Examples

**Hybrid Mode (Cache + API)**:
```bash
# Default: qwen-flash
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache
```

**API-Only Mode (No Cache)**:
```bash
# qwen-flash (fastest, cheapest)
python run_gsm8k_cache_API.py --llm_name qwen-flash

# qwen-plus (more capable)
python run_gsm8k_cache_API.py --llm_name qwen-plus

# qwen-turbo (alternative)
python run_gsm8k_cache_API.py --llm_name qwen-turbo
```

---

## ğŸ® Generation Modes

### Three Ways to Generate Text

#### 1. API_HINT (Default)

**Method**: `generate_text_batch_api`

```bash
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache --generation_mode api_hint
```

**Flow**: Cache â†’ Text hint â†’ API

**Pros**: Simple | **Cons**: No real cache usage

#### 2. HYBRID (Best Quality) â­

**Method**: `generate_text_batch_hybrid`

```bash
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache --generation_mode hybrid
```

**Flow**: Cache â†’ Local model (uses cache) â†’ API refines

**Pros**: Real cache + API quality | **Cons**: Slower

#### 3. LOCAL (Pure Local)

**Method**: `generate_text_batch`

```bash
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache --generation_mode local
```

**Flow**: Cache â†’ Local model (uses cache) â†’ Output

**Pros**: Real cache, no API cost | **Cons**: Lower quality

### Comparison

| Mode | Cache Usage | Quality | Speed | API Cost |
|------|-------------|---------|-------|----------|
| **api_hint** | Text hint | Good | Medium | Yes |
| **hybrid** | Real tensors | **Best** â­ | Slow | Yes |
| **local** | Real tensors | OK | Fast | No |

---

## ğŸ› Bug Fixes

### 1. Missing `get_edge_weight` Method
**Error:** `'GCN' object has no attribute 'get_edge_weight'`

**Location:** `GDesigner/gnn/gcn.py`

**Fix:** Added the missing method to the GCN class:
```python
def get_edge_weight(self, src_id, dst_id):
    """Get edge weight between two nodes (returns 1.0 as default)"""
    return 1.0
```

### 2. Gradient Flow Issue
**Error:** `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn`

**Root Cause:** The `spatial_logits` tensor was being reassigned in `arun()` method, breaking the gradient connection to the GCN parameters.

**Location:** `GDesigner/graph/graph.py` (line ~327)

**Fix:** Changed from:
```python
self.spatial_logits = min_max_norm(torch.flatten(self.spatial_logits))
```

To:
```python
spatial_logits_matrix = logits @ logits.t()
self.spatial_logits = torch.nn.Parameter(
    min_max_norm(torch.flatten(spatial_logits_matrix)),
    requires_grad=self.optimized_spatial
)
```

This ensures the tensor maintains gradient tracking and is properly registered as a Parameter.

### 3. Training Configuration
**Location:** `experiments/run_gsm8k_cache_API.py`

**Changes:**
- Set `optimized_spatial` default to `True` (required for cache training)
- Simplified backprop condition to only check `args.use_cache`

### Key Points

1. **Gradient Flow:** The GCN â†’ MLP â†’ spatial_logits â†’ log_probs chain now properly maintains gradients
2. **Cache Training:** When `--use_cache` is enabled, both GCN and CacheFuser parameters are optimized
3. **Edge Weights:** The `get_edge_weight` method provides default weights (can be extended for learned edge weights)


The API call flow 
FinalRefer â†’ HybridCacheLLM.agen() â†’ hybrid_model.generate_text_batch_api() 
â†’ AsyncOpenAI.chat.completions.create(model="qwen-flash", max_tokens=4096)
â†’ Your .env BASE_URL + API_KEY


Step-by-Step Call Chain:
1. run_gpqa_cache_API.py (line ~200)
   answer_log_probs.append(asyncio.create_task(graph.arun(input_dict, args.num_rounds)))
   â†“

2. graph.arun() (graph.py line 374)
   - Constructs graph connections
   - Executes nodes in topological order
   - Line 402: await self.nodes[current_node_id].async_execute(input)
   â†“

3. node.async_execute() (node.py line 157)
   - Gets spatial_info and temporal_info
   - Line 162: self._async_execute(input, spatial_info, temporal_info)
   â†“

4. MathSolverCache._async_execute() (math_solver_cache.py line 101)
   - Your STEP 3-11 logs
   - Line 175: return response
   â†“

5. Back to node.async_execute() (node.py line 168)
   - Stores result in self.outputs
   - Returns self.outputs
   â†“

6. Back to graph.arun() (graph.py line 418-420)
   - After all nodes execute, calls decision_node
   - Returns: final_answers, log_probs
   â†“

7. Back to run_gpqa_cache_API.py (line ~210)
   raw_results = await asyncio.gather(*answer_log_probs)
   raw_answers, log_probs = zip(*raw_results)




# CacheDesigner Workflow Tracking Guide

## Complete Execution Flow with Step Numbers

### High-Level Overview
```
User Task â†’ Graph Execution â†’ Multi-Agent Processing â†’ Cache Communication â†’ Final Response
```

### Detailed Step-by-Step Flow

#### **STEP 1-2: Graph Initialization**
- **Location:** `cache_graph.py:169`
- **Print:** `ğŸš€ [STEP 1] CacheGraph.arun() - Starting graph execution`
- **What happens:** 
  - Clears node caches
  - Sets graph reference on all nodes
  - Calls parent Graph.arun()

#### **STEP 3: Agent Execution Starts**
- **Location:** `math_solver_cache.py:113`
- **Print:** `ğŸ¯ [STEP 3] MathSolverCache._async_execute() - Executing node {node_id}`
- **What happens:** Agent begins processing its task

#### **STEP 4: Cache Retrieval**
- **Location:** `cache_graph.py:127`
- **Print:** `ğŸ”„ [STEP 4] CacheGraph.get_fused_cache() - Getting fused cache for node {node_id}`
- **What happens:** 
  - Checks for spatial predecessors
  - Fuses caches from predecessor nodes
  - Returns None for root nodes (no predecessors)

#### **STEP 5: Cache Status**
- **Location:** `math_solver_cache.py:118`
- **Print:** `ğŸ“¥ [STEP 5] MathSolverCache - Received fused cache: {True/False}`
- **What happens:** Agent receives (or doesn't receive) cache from predecessors

#### **STEP 5a: Prompt Building**
- **Location:** `math_solver_cache.py:121`
- **Print:** `ğŸ“ [STEP 5a] MathSolverCache._process_inputs() - Building prompt with context`
- **What happens:**
  - Combines task + spatial context + temporal context
  - Builds system and user prompts
  - Creates messages array

#### **STEP 6: LLM Call**
- **Location:** `math_solver_cache.py:126`
- **Print:** `ğŸ§  [STEP 6] MathSolverCache - Calling llm.agen_with_cache() (Goal: Generate reasoning cache + text response)`
- **What happens:** Agent calls LLM to generate response with cache

#### **STEP 7: Cache Generation Starts**
- **Location:** `llm_cache_hybrid.py:81`
- **Print:** `ğŸ“¦ [STEP 7] HybridCacheLLM.agen_with_cache() - Starting cache generation`
- **What happens:** LLM wrapper begins processing

#### **STEP 7a: Message Conversion**
- **Location:** `llm_cache_hybrid.py:84`
- **Print:** `ğŸ’¬ [STEP 7a] Converting messages to text prompt...`
- **What happens:** Converts chat messages to text format

#### **STEP 7b: Tokenization**
- **Location:** `llm_cache_hybrid.py:88`
- **Print:** `ğŸ”¤ [STEP 7b] Tokenizing prompt...`
- **What happens:**
  - Clears CUDA cache
  - Tokenizes prompt
  - Validates token IDs
  - Moves tensors to device

#### **STEP 8: Latent Cache Generation**
- **Location:** `llm_cache_hybrid.py:135`
- **Print:** `ğŸ”— [STEP 8] HybridCacheLLM - Calling hybrid_model.generate_latent_batch()`
- **What happens:** Calls model to generate KV-cache

#### **STEP 8a: Cache Model Forward Pass**
- **Location:** `hybrid_cache_model.py:102`
- **Print:** `ğŸ§  [STEP 8a] HybridCacheModel.generate_latent_batch() - Generating cache using small local model`
- **What happens:**
  - Forward pass through local model
  - 10 latent reasoning steps
  - Builds KV-cache (no text yet!)

#### **STEP 9: Text Generation**
- **Location:** `llm_cache_hybrid.py:145`
- **Print:** `ğŸ“ [STEP 9] HybridCacheLLM - Generating text with mode: {mode}`
- **What happens:** Chooses generation mode (hybrid/local/api_hint)

#### **STEP 9a: Local Model Generation** (if mode=local or hybrid)
- **Location:** `hybrid_cache_model.py:54`
- **Print:** `ğŸ¯ [STEP 9a] HybridCacheModel.generate_text_batch() - Generating text using cache TENSORS directly`
- **What happens:**
  - Uses KV-cache from STEP 8
  - Calls HuggingFace model.generate()
  - Generates text tokens autoregressively

#### **STEP 9b: API Generation** (if mode=api_hint)
- **Location:** `hybrid_cache_model.py:130`
- **Print:** `ğŸ”„ [STEP 9b] HybridCacheModel.generate_text_batch_api() - Converting cache to text context`
- **What happens:**
  - Converts cache to text hint
  - Calls API (qwen-plus/qwen-flash)
  - Returns API response

#### **STEP 10: Cache Storage**
- **Location:** `math_solver_cache.py:136`
- **Print:** `ğŸ’¾ [STEP 10] MathSolverCache - Calling store_node_cache() for {n} successors`
- **What happens:** Stores generated cache for successor nodes to use

#### **STEP 11: Response Return**
- **Location:** `math_solver_cache.py:141`
- **Print:** `ğŸ¯ [STEP 11] MathSolverCache - Returning final response from node {node_id}`
- **What happens:** Agent returns its final text response

#### **STEP 12: Graph Aggregation**
- **Location:** `run_gsm8k_cache_API.py:175`
- **Print:** `â³ [STEP 12] Waiting for graph.arun() to complete for {n} tasks...`
- **What happens:** Waits for all agents to finish

#### **STEP 13: Results Collection**
- **Location:** `run_gsm8k_cache_API.py:178`
- **Print:** `ğŸ [STEP 13] Graph execution complete - received {n} responses`
- **What happens:** 
  - Collects all agent responses
  - Shows preview of each response
  - **THIS IS WHERE YOUR OUTPUT COMES FROM!**

#### **STEP 14: Metrics Computation**
- **Location:** `run_gsm8k_cache_API.py:183`
- **Print:** `ğŸ“Š [STEP 14] Processing results and computing metrics...`
- **What happens:**
  - Extracts predicted answers
  - Computes accuracy
  - Saves results to JSON

---

## Understanding Your Output

The output you see:
```
The task is: Kylar went to the store...
At the same time, the output of other agents is as follows:
7LdC: Let's solve this step by step...
5qxQ: Let's analyze the problem...
...
################response:After reviewing the analysis...
```

**Comes from STEP 13** - This is the **final aggregated response** from the graph after all agents have completed.

### Where Each Part Comes From:

1. **"The task is:"** - From the prompt template in `_process_inputs()` (STEP 5a)
2. **"output of other agents"** - From spatial context (other agents in same round)
3. **"7LdC:", "5qxQ:", etc.** - Individual agent responses (STEP 11 from each agent)
4. **"################response:"** - Final decision/aggregation (from decision agent or final node)

### Is it from Cache or Direct Model?

The text is generated in **STEP 9**:
- **If mode=local:** Direct from local model using cache tensors (STEP 9a)
- **If mode=api_hint:** From API with cache converted to text hint (STEP 9b)
- **If mode=hybrid:** Local model first, then API refinement

The cache itself (KV-cache tensors) is generated in **STEP 8a** and contains the "reasoning" in latent space, but the actual text you see is generated in **STEP 9**.

---

## Quick Reference

| Step | Component | What You See |
|------|-----------|--------------|
| 1-2 | Graph Init | Graph setup messages |
| 3-6 | Agent Setup | Agent execution start |
| 7-7b | Tokenization | Prompt processing |
| 8-8a | Cache Gen | KV-cache creation (latent reasoning) |
| 9-9b | Text Gen | **Actual text generation** |
| 10-11 | Cache Store | Cache saved for next nodes |
| 12-13 | Aggregation | **Final output collection** |
| 14 | Metrics | Accuracy computation |

**Your output appears at STEP 13** - it's the final aggregated response after all agents complete their processing.


æˆ‘æ¥è¯¦ç»†è§£é‡ŠTransformeræ¨¡å‹ä¸­çš„forward passå’Œgenerationè¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ‰€æœ‰ç»´åº¦ä¿¡æ¯ã€‚

1. Hidden State H(l) çš„è®¡ç®—è¿‡ç¨‹
H(l) âˆˆ R^(1Ã—690Ã—2560) æ˜¯è¿™æ ·å¾—åˆ°çš„ï¼š

å®Œæ•´çš„Transformer Layerè®¡ç®—ï¼š

è¾“å…¥: H(l-1) âˆˆ R^(BÃ—TÃ—D)  (B=1, T=690, D=2560)

æ­¥éª¤1: Self-Attention
------
Q = H(l-1) @ W_Q  â†’  Q âˆˆ R^(1Ã—690Ã—2560)
K = H(l-1) @ W_K  â†’  K âˆˆ R^(1Ã—690Ã—2560)  
V = H(l-1) @ W_V  â†’  V âˆˆ R^(1Ã—690Ã—2560)

æ­¥éª¤2: Multi-Head Attention (å‡è®¾32ä¸ªheads)
------
å°†Q,K,Våˆ†æˆ32ä¸ªheads:
Q_heads âˆˆ R^(1Ã—32Ã—690Ã—80)  (2560/32=80)
K_heads âˆˆ R^(1Ã—32Ã—690Ã—80)
V_heads âˆˆ R^(1Ã—32Ã—690Ã—80)

æ­¥éª¤3: Scaled Dot-Product Attention
------
scores = (Q_heads @ K_heads^T) / sqrt(80)  â†’  scores âˆˆ R^(1Ã—32Ã—690Ã—690)

æ­¥éª¤4: Apply Attention Mask (è¿™é‡Œå›ç­”ä½ çš„maské—®é¢˜)
------
# attention_mask âˆˆ R^(1Ã—690), å€¼ä¸º0æˆ–1
# æ‰©å±•ä¸º R^(1Ã—1Ã—690Ã—690) çš„causal mask
mask = (1 - attention_mask) * (-10000)  # 0å˜0, 1å˜-10000
scores = scores + mask  # Element-wiseåŠ æ³•!

æ­¥éª¤5: Softmax (åœ¨sequenceç»´åº¦ä¸Š)
------
attn_weights = softmax(scores, dim=-1)  # åœ¨æœ€åä¸€ç»´(690)ä¸Šåšsoftmax
# attn_weights âˆˆ R^(1Ã—32Ã—690Ã—690)

æ­¥éª¤6: Apply to Values
------
attn_output = attn_weights @ V_heads  â†’  R^(1Ã—32Ã—690Ã—80)

æ­¥éª¤7: Concat heads
------
attn_output = concat(attn_output)  â†’  R^(1Ã—690Ã—2560)

æ­¥éª¤8: Output projection
------
O = attn_output @ W_O  â†’  O âˆˆ R^(1Ã—690Ã—2560)

æ­¥éª¤9: Add & Norm
------
H_attn = LayerNorm(H(l-1) + O)  â†’  R^(1Ã—690Ã—2560)

æ­¥éª¤10: FFN
------
H(l) = LayerNorm(H_attn + FFN(H_attn))  â†’  R^(1Ã—690Ã—2560)


Copy
æ‰€ä»¥æ˜¯çš„ï¼Œéœ€è¦QKVï¼ æ¯ä¸€å±‚éƒ½è¦è®¡ç®—QKVã€‚

2. Attention Mask çš„ä½œç”¨æœºåˆ¶
# attention_mask ç¤ºä¾‹: [1, 1, 1, 0, 0]  (å‰3ä¸ªtokenæœ‰æ•ˆï¼Œå2ä¸ªpadding)

# æ­¥éª¤1: è½¬æ¢ä¸ºadditive mask
mask = (1 - attention_mask) * (-10000)
# ç»“æœ: [0, 0, 0, -10000, -10000]

# æ­¥éª¤2: æ‰©å±•åˆ°attention scoresç»´åº¦
# scores âˆˆ R^(BÃ—headsÃ—TÃ—T)
# maskæ‰©å±•ä¸º R^(BÃ—1Ã—1Ã—T) ç„¶åbroadcast

# æ­¥éª¤3: Element-wise åŠ æ³• (ä¸æ˜¯ä¹˜æ³•!)
scores = scores + mask
# ä¾‹å¦‚æŸä¸ªä½ç½®çš„scoreæ˜¯5.2ï¼Œå¦‚æœmaskæ˜¯-10000ï¼Œåˆ™å˜æˆ-9994.8

# æ­¥éª¤4: Softmax
attn_weights = softmax(scores, dim=-1)
# exp(-10000) â‰ˆ 0ï¼Œæ‰€ä»¥è¢«maskçš„ä½ç½®æƒé‡â‰ˆ0

å…³é”®ç‚¹ï¼š

Maskæ˜¯ åŠ æ³• ï¼Œä¸æ˜¯ä¹˜æ³•

Softmaxæ˜¯åœ¨ sequence level ï¼ˆæœ€åä¸€ç»´ï¼‰ä¸Šåšçš„

è¢«maskçš„ä½ç½®ç»è¿‡softmaxåæƒé‡â‰ˆ0ï¼Œç›¸å½“äº"knock out"

3. Generationè¿‡ç¨‹è¯¦è§£
Prefillé˜¶æ®µï¼ˆç¬¬ä¸€æ¬¡forwardï¼‰

è¾“å…¥: input_ids âˆˆ R^(1Ã—T)  ä¾‹å¦‚ T=10
past_kv = None

æ­¥éª¤1: Embedding
------
H(0) = Embedding(input_ids)  â†’  H(0) âˆˆ R^(1Ã—10Ã—2560)

æ­¥éª¤2: é€šè¿‡æ‰€æœ‰Transformer layers
------
for layer in layers:
    H(l) = TransformerLayer(H(l-1))  # å¦‚ä¸Šé¢è¯¦ç»†è¿‡ç¨‹
    # åŒæ—¶ç”Ÿæˆå¹¶ä¿å­˜ K(l), V(l) âˆˆ R^(1Ã—32Ã—10Ã—80)

æœ€ç»ˆ: H(L) âˆˆ R^(1Ã—10Ã—2560)  (Læ˜¯æœ€åä¸€å±‚)

æ­¥éª¤3: å–æœ€åä¸€ä¸ªtokençš„hidden state
------
h_last = H(L)[:, -1, :]  â†’  h_last âˆˆ R^(1Ã—2560)

æ­¥éª¤4: ç”Ÿæˆlogits
------
logits = h_last @ W_lm_head  â†’  logits âˆˆ R^(1Ã—vocab_size)

æ­¥éª¤5: é‡‡æ ·next token
------
next_token_id = sample(logits)  â†’  scalar

æ­¥éª¤6: ä¿å­˜KV cache
------
past_kv = [(K(1), V(1)), (K(2), V(2)), ..., (K(L), V(L))]
# æ¯ä¸ª K(l), V(l) âˆˆ R^(1Ã—32Ã—10Ã—80)

Autoregressiveé˜¶æ®µï¼ˆåç»­ç”Ÿæˆï¼‰

è¾“å…¥: next_token_id (scalar)
past_kv = [(K, V) for each layer]  # K,V âˆˆ R^(1Ã—32Ã—T_pastÃ—80)

æ­¥éª¤1: Embedding
------
h_new = Embedding(next_token_id)  â†’  h_new âˆˆ R^(1Ã—1Ã—2560)

æ­¥éª¤2: é€šè¿‡æ‰€æœ‰layers (ä½¿ç”¨KV cache!)
------
for layer_idx, layer in enumerate(layers):
    K_past, V_past = past_kv[layer_idx]  # R^(1Ã—32Ã—T_pastÃ—80)
    
    # åªè®¡ç®—æ–°tokençš„Q,K,V
    Q_new = h_new @ W_Q  â†’  R^(1Ã—32Ã—1Ã—80)
    K_new = h_new @ W_K  â†’  R^(1Ã—32Ã—1Ã—80)
    V_new = h_new @ W_V  â†’  R^(1Ã—32Ã—1Ã—80)
    
    # Concat with past
    K_full = concat([K_past, K_new], dim=2)  â†’  R^(1Ã—32Ã—(T_past+1)Ã—80)
    V_full = concat([V_past, V_new], dim=2)  â†’  R^(1Ã—32Ã—(T_past+1)Ã—80)
    
    # Attention (Qåªæœ‰1ä¸ªtokenï¼Œä½†attend toæ‰€æœ‰past tokens)
    scores = Q_new @ K_full^T  â†’  R^(1Ã—32Ã—1Ã—(T_past+1))
    attn = softmax(scores, dim=-1)
    output = attn @ V_full  â†’  R^(1Ã—32Ã—1Ã—80)
    
    # ç»§ç»­FFNç­‰...
    h_new = TransformerLayer(h_new, K_full, V_full)  â†’  R^(1Ã—1Ã—2560)
    
    # æ›´æ–°cache
    past_kv[layer_idx] = (K_full, V_full)

æ­¥éª¤3: ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
------
h_last = h_new[:, -1, :]  â†’  R^(1Ã—2560)
logits = h_last @ W_lm_head  â†’  R^(1Ã—vocab_size)
next_token = sample(logits)

4. å…³äºä½ çš„ä»£ç ä¸­çš„latent generation
åœ¨ä½ çš„ä»£ç ä¸­ï¼š

# generate_latent_batch() - åªç”Ÿæˆcacheï¼Œä¸ç”Ÿæˆtoken
outputs = self.cache_model(
    inputs_embeds=latent_embed,  # R^(1Ã—1Ã—2560)
    past_key_values=past,
    use_cache=True,
    output_hidden_states=True,
    return_dict=True,
)
# æ³¨æ„: è¿™é‡Œæ²¡æœ‰è°ƒç”¨ .generate()
# æ‰€ä»¥æ²¡æœ‰ outputs.sequences å±æ€§!
# åªæœ‰ outputs.hidden_states å’Œ outputs.past_key_values

ä½ çš„é”™è¯¯åŸå› ï¼š

output_seq_len = outputs.sequences.shape[1]  # âŒ é”™è¯¯!
# CausalLMOutputWithPast æ²¡æœ‰ sequences å±æ€§
# sequences åªåœ¨ .generate() çš„è¿”å›å€¼ä¸­å­˜åœ¨



å¥½é—®é¢˜ï¼è®©æˆ‘è§£é‡Šä¸€ä¸‹å®ƒä»¬çš„å…³ç³»ï¼š

MathAgentCacheV2 vs MathSolverCacheV2
1. ç»§æ‰¿å…³ç³»
MathAgentCacheV2 ç»§æ‰¿è‡ª DiverseAgentCacheV2 ï¼ˆåŸºç±»ï¼‰ï¼Œè€Œä¸æ˜¯ MathSolverCacheV2ï¼š

# diverse_agents_cache_v2.py
class DiverseAgentCacheV2(Node):  # â† åŸºç±»
    """Base class for diverse cache-enabled agents"""
    # ... æ‰€æœ‰æ ¸å¿ƒé€»è¾‘åœ¨è¿™é‡Œ ...

class MathAgentCacheV2(DiverseAgentCacheV2):  # â† ç»§æ‰¿åŸºç±»
    """Math Solver Agent"""
    def __init__(self, id: str = None, domain: str = "", llm_name: str = "", **kwargs):
        super().__init__(id, "MathAgentCacheV2", domain, llm_name, 
                        role="Math Solver", **kwargs)  # â† åªè®¾ç½®role

Copy
python
2. ä»£ç å¤ç”¨
MathAgentCacheV2 å’Œ MathSolverCacheV2 çš„ _async_execute é€»è¾‘å®Œå…¨ç›¸åŒï¼

æˆ‘æŠŠ MathSolverCacheV2 çš„æ ¸å¿ƒé€»è¾‘å¤åˆ¶åˆ°äº† DiverseAgentCacheV2 åŸºç±»ä¸­ï¼š

# DiverseAgentCacheV2 (åŸºç±») åŒ…å«å®Œæ•´çš„ _async_execute é€»è¾‘
async def _async_execute(self, input, spatial_info, temporal_info):
    # Step 1: Get fused cache
    past_kv = graph.get_fused_cache(self)
    
    # Step 2: Build prompt
    system_prompt, user_prompt = self._process_inputs(...)
    
    # Step 3: Generate with cache
    response, kv_cache = await self.llm.agen_with_cache(...)
    
    # Step 4: Truncate cache if latent_only
    if self.latent_only:
        kv_cache = self._truncate_past(kv_cache, self.latent_steps)
    
    # Step 5: Store cache
    graph.store_node_cache(self.id, kv_cache)
    
    # Step 6: Return response
    return response if self.agent_type != "intermediate" else ""

Copy
python
3. å”¯ä¸€çš„åŒºåˆ«ï¼šRole
Agent	ç»§æ‰¿è‡ª	Role	Prompt
MathSolverCacheV2	Node	"Math Solver" (é»˜è®¤)	æ•°å­¦ä¸“å®¶
MathAgentCacheV2	DiverseAgentCacheV2	"Math Solver"	æ•°å­¦ä¸“å®¶
AnalystAgentCacheV2	DiverseAgentCacheV2	"Mathematical Analyst"	æ•°å­¦åˆ†æå¸ˆ
CodeAgentCacheV2	DiverseAgentCacheV2	"Programming Expert"	ç¼–ç¨‹ä¸“å®¶
InspectorAgentCacheV2	DiverseAgentCacheV2	"Inspector"	æ£€æŸ¥å‘˜
4. ä¾èµ–å…³ç³»
æ²¡æœ‰ä¾èµ–å…³ç³»ï¼ å®ƒä»¬æ˜¯å¹³è¡Œçš„ï¼š

Node (åŸºç±»)
â”œâ”€â”€ MathSolverCacheV2 (ç‹¬ç«‹å®ç°)
â””â”€â”€ DiverseAgentCacheV2 (åŸºç±»)
    â”œâ”€â”€ MathAgentCacheV2
    â”œâ”€â”€ AnalystAgentCacheV2
    â”œâ”€â”€ CodeAgentCacheV2
    â””â”€â”€ InspectorAgentCacheV2

Copy
5. ä¸ºä»€ä¹ˆè¦åˆ›å»ºæ–°çš„ï¼Ÿ
åŸå› ï¼šä»£ç å¤ç”¨ + å¤šæ ·åŒ–

MathSolverCacheV2 : å•ä¸€ç”¨é€”ï¼Œæ‰€æœ‰agentséƒ½æ˜¯Math Solver

DiverseAgentCacheV2 : å¯å¤ç”¨åŸºç±»ï¼Œæ”¯æŒ4ç§ä¸åŒè§’è‰²

å¥½å¤„ï¼š

é¿å…ä»£ç é‡å¤ï¼ˆ4ä¸ªagentså…±äº«åŒä¸€ä¸ª _async_executeï¼‰

æ˜“äºç»´æŠ¤ï¼ˆä¿®æ”¹ä¸€æ¬¡ï¼Œ4ä¸ªagentséƒ½æ›´æ–°ï¼‰

æ”¯æŒå¤šæ ·åŒ–ï¼ˆæ¯ä¸ªagentæœ‰ä¸åŒçš„roleå’Œpromptï¼‰

6. å®é™…æ‰§è¡Œæµç¨‹ç›¸åŒ
æ— è®ºæ˜¯ MathSolverCacheV2 è¿˜æ˜¯ MathAgentCacheV2ï¼Œæ‰§è¡Œæµç¨‹å®Œå…¨ä¸€æ ·ï¼š

# ä¸¤è€…éƒ½æ‰§è¡Œç›¸åŒçš„æ­¥éª¤ï¼š
1. Get fused cache from predecessors
2. Build prompt (å”¯ä¸€åŒºåˆ«ï¼šroleä¸åŒï¼Œpromptä¸åŒ)
3. Generate with cache
4. Truncate cache if latent_only
5. Store cache for successors
6. Return response

Copy
python
æ€»ç»“
æ²¡æœ‰ä¾èµ–å…³ç³» - MathAgentCacheV2 ä¸ä¾èµ– MathSolverCacheV2

ä»£ç é€»è¾‘ç›¸åŒ - æˆ‘æŠŠ MathSolverCacheV2 çš„é€»è¾‘å¤åˆ¶åˆ°äº† DiverseAgentCacheV2

å”¯ä¸€åŒºåˆ« - Role å’Œ Prompt ä¸åŒ

ä¼˜åŠ¿ - ä»£ç å¤ç”¨ + æ”¯æŒå¤šæ ·åŒ–agents

ä½ å¯ä»¥æŠŠ MathAgentCacheV2 çœ‹ä½œæ˜¯ MathSolverCacheV2 çš„"å¤šæ ·åŒ–ç‰ˆæœ¬"ï¼Œå®ƒä»¬åŠŸèƒ½ç›¸åŒï¼Œä½†æ”¯æŒä¸åŒçš„è§’è‰²ï¼




# G-Designerï¼ˆè¿™ä»½ä»£ç ï¼‰çš„å®Œæ•´è®¡ç®—å›¾ï¼ˆå¯¹é½å®ç°ï¼‰

## 0) è®°å·

- èŠ‚ç‚¹æ•°ï¼š`N`
- æŸ¥è¯¢/é¢˜ç›®ï¼š`q`
- æ¯ä¸ªèŠ‚ç‚¹çš„è§’è‰²æè¿° embeddingï¼ˆå›ºå®šï¼‰ï¼š`x_i âˆˆ R^d`
- æŸ¥è¯¢ embeddingï¼š`e(q) âˆˆ R^d`
- å›ºå®š role å…ˆéªŒå›¾ï¼š`A_role`ï¼ˆç”¨äº GCNï¼‰
- å›ºå®šå€™é€‰è¾¹ maskï¼š`M âˆˆ {0,1}^(NÃ—N)`ï¼ˆç”± mode ç”Ÿæˆï¼Œä¾‹å¦‚ FullConnected/Chain/Starï¼‰
- é‡‡æ ·å¾—åˆ°çš„æ‰§è¡Œå›¾è¾¹ï¼š`a_ij âˆˆ {0,1}`

---

## 1) å‰å‘ï¼ˆå¯å¯¼éƒ¨åˆ†ï¼šäº§ç”Ÿè¾¹ logits / æ¦‚ç‡ï¼‰

### (1) å›ºå®šèŠ‚ç‚¹ç‰¹å¾

å¯¹æ¯ä¸ªèŠ‚ç‚¹ `i`ï¼Œå…ˆç”¨ role çš„ profile å¾—åˆ° embeddingï¼š

```
x_i = Embed(role_profile_i)
```

ç»„æˆçŸ©é˜µï¼š

```
X = [x_1; ...; x_N] âˆˆ R^(NÃ—d)
```

### (2) æŸ¥è¯¢ç‰¹å¾æ‹¼æ¥ï¼ˆæ¯é¢˜ä¸åŒï¼‰

```
e(q) = Embed(q)
```

å¤åˆ¶åˆ°æ¯ä¸ªèŠ‚ç‚¹ï¼š

```
E(q) = 1_N âŠ— e(q)^T âˆˆ R^(NÃ—d)
```

æ‹¼æ¥å¾—åˆ°ï¼š

```
X' = [X || E(q)] âˆˆ R^(NÃ—2d)
```

ï¼ˆå¯¹åº”ä»£ç ï¼š`construct_new_features`ï¼‰

### (3) å›ºå®šå›¾ä¸Šçš„ GCN æ¶ˆæ¯ä¼ é€’

```
H = GCN_Î¸(X', A_role) âˆˆ R^(NÃ—d)
```

ï¼ˆå¯¹åº”ä»£ç ï¼š`self.gcn(new_features, self.role_adj_matrix)`ï¼‰

### (4) MLP æ˜ å°„ï¼ˆæ³¨æ„ï¼šä»£ç é‡Œç”¨äº†ï¼Œä½†è®­ç»ƒè„šæœ¬é»˜è®¤æ²¡æ›´æ–°å®ƒï¼‰

```
Z = MLP(H) âˆˆ R^(NÃ—k)
```

ï¼ˆå¯¹åº”ä»£ç ï¼š`logits = self.mlp(logits)`ï¼‰

### (5) è¾¹æ‰“åˆ†ï¼ˆä¸¤ä¸¤ç‚¹ç§¯ï¼‰

```
S = Z @ Z^T âˆˆ R^(NÃ—N)
s_ij = z_i^T @ z_j
```

ï¼ˆå¯¹åº”ä»£ç ï¼š`self.spatial_logits = logits @ logits.t()`ï¼‰

### (6) min-max å½’ä¸€åŒ–åˆ° `[-1,1]`ï¼ˆä»£ç æœ‰è¿™ä¸€æ­¥ï¼‰


```

sÌƒ_ij = 2Ã— \frac{s_ij-\min(S)}{\max(S)-\min(S)} - 1

```


ï¼ˆå¯¹åº”ä»£ç ï¼š`min_max_norm(torch.flatten(...))`ï¼‰

### (7) å˜æˆæ¦‚ç‡


```

p_ij=Ïƒ(sÌƒ_ij/T)

```


ï¼ˆå¯¹åº”ä»£ç ï¼š`edge_prob = sigmoid(edge_logit / temperature)`ï¼‰

> åˆ°è¿™é‡Œå…¨éƒ¨å¯å¯¼ï¼ˆå¯¹ GCN/MLP å‚æ•°å¯å¯¼ï¼‰ã€‚

---

## 2) é‡‡æ ·ï¼ˆä¸å¯å¯¼ï¼Œä½†æ²¡å…³ç³»ï¼šREINFORCEï¼‰

å¯¹æ¯æ¡å€™é€‰è¾¹ `(i,j)`ï¼š

### (A) mask çº¦æŸï¼ˆå†³å®šâ€œè¿™æ¡è¾¹æ˜¯å¦å…è®¸å­˜åœ¨â€ï¼‰

- è‹¥ `M_ij=0`ï¼šç›´æ¥è·³è¿‡ï¼ˆæ°¸ä¸å‡ºç°ï¼‰
- è‹¥ `M_ij=1`ï¼šæ‰è€ƒè™‘è¿™æ¡è¾¹

ï¼ˆå¯¹åº”ä»£ç ï¼š`if edge_mask == 0.0: continue`ï¼‰

### (B) ä¸¤ç§æ¨¡å¼ï¼šæ˜¯å¦çœŸçš„é‡‡æ ·

#### æƒ…å†µ 1ï¼š`optimized_spatial == False`ï¼ˆé»˜è®¤å¾ˆå¤šå®éªŒæ˜¯è¿™ä¸ªï¼‰

- mask=1 çš„è¾¹ **ç›´æ¥å¿…è¿**ï¼ˆåªè¦ä¸é€ æˆ cycleï¼‰
- è¿™æ—¶å›¾ç»“æ„ä¸å­¦ä¹ ï¼ŒGCN è¾“å‡ºçš„ `p_ij` å…¶å®ä¸å½±å“è¿æ¥

ï¼ˆå¯¹åº”ä»£ç ï¼š`elif edge_mask==1 and optimized_spatial==False: add_successor`ï¼‰

#### æƒ…å†µ 2ï¼š`optimized_spatial == True`

æ‰ä¼šçœŸçš„è¿›è¡Œ Bernoulli é‡‡æ ·ï¼š


```

a_ij~ Bernoulli(p_ij)

```


å¹¶ä¸”è¿˜ä¼šåšä¸€ä¸ª spatial cycle-checkï¼ˆé¿å…æœ‰å‘ç¯ï¼‰ã€‚

---

## 3) è®¡ç®— log-probï¼ˆå…³é”®ï¼šæ¢¯åº¦ä»è¿™é‡Œå›ä¼ ï¼‰

é‡‡æ ·åˆ°çš„æ•´å¼ æ‰§è¡Œå›¾ `G` çš„ log æ¦‚ç‡ï¼ˆå¯¹é½ä»£ç ï¼‰ï¼š

å¯¹æ¯æ¡â€œå‚ä¸é‡‡æ ·çš„è¾¹â€ï¼ˆmask=1 ä¸” optimized_spatial=True ä¸”é€šè¿‡ cycle check çš„è¾¹ï¼‰ï¼š

- å¦‚æœ `a_ij=1`ï¼šåŠ  `log p_ij`
- å¦‚æœ `a_ij=0`ï¼šåŠ  `log(1-p_ij)`

æ‰€ä»¥ï¼š


```

log Ï€_Î¸(G)
=Î£_ij[a_ijlog p_ij+(1-a_ij)log(1-p_ij)]

```


ï¼ˆå¯¹åº”ä»£ç ï¼š`log_probs.append(log(p))` / `log(1-p)`ï¼‰

> è¿™ä¸€æ­¥å¯å¯¼ï¼Œå› ä¸º `p_ij=Ïƒ(Ã—)` å¯å¯¼ï¼Œ`log` å¯å¯¼ã€‚  
> é‡‡æ ·åŠ¨ä½œ `a_ij` ä¸å¯å¯¼ï¼Œä½† REINFORCE ä¸éœ€è¦å®ƒå¯å¯¼ã€‚

---

## 4) æ‰§è¡Œå›¾ä¸Šçš„å¤š Agent æ¨ç†ï¼ˆä¸å¯å¯¼ï¼‰

æ ¹æ®é‡‡æ ·å¾—åˆ°çš„ spatial predecessors/successorsï¼š

- æ‹“æ‰‘æ’åºæ‰§è¡Œå„èŠ‚ç‚¹ `async_execute`
- æ¯ä¸ªèŠ‚ç‚¹æŠŠ predecessor çš„ outputs æ‹¼è¿› prompt
- è°ƒç”¨å¤–éƒ¨ LLMï¼ˆä¸å¯å¯¼ï¼‰ç”Ÿæˆæ–‡æœ¬è¾“å‡º
- decision node æ±‡æ€»è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ

ï¼ˆå¯¹åº”ï¼š`Graph.arun()` topo loop + `Node.get_spatial_info/get_temporal_info` + agent `_async_execute`ï¼‰

---

## 5) rewardï¼ˆä¸å¯å¯¼ï¼‰ä¸ lossï¼ˆå¯å¯¼åˆ° GNNï¼‰

å¯¹ GSM8Kï¼š


```

R=1[parsed_answer=true_answer]âˆˆ\{0,1\}

```


lossï¼ˆå¯¹é½è®­ç»ƒè„šæœ¬ï¼‰ï¼š


```

L(Î¸)=-RÃ— log Ï€_Î¸(G)

```


- è‹¥ç­”å¯¹ `R=1`ï¼šæ¨åŠ¨è¿™æ¬¡é‡‡æ ·åˆ°çš„å›¾æ›´å¯èƒ½å†æ¬¡è¢«é‡‡åˆ°
- è‹¥ç­”é”™ `R=0`ï¼šloss=0ï¼ˆè¿™ä»½å®ç°é‡Œä¸æ›´æ–°ï¼‰

ï¼ˆå¯¹åº”ä»£ç ï¼š`single_loss = -log_prob * utility`ï¼‰

---

## 6) åå‘ä¼ æ’­æ›´æ–°åœ¨å“ªé‡Œï¼Ÿæ›´æ–°å‡ æ¬¡ï¼Ÿ

æ¯ä¸ª batchï¼š

```python
total_loss.backward()
optimizer.step()



================================================================================
æ­¥éª¤ 1: è®¡ç®—è¾¹çš„ logits çŸ©é˜µ
================================================================================
èŠ‚ç‚¹åµŒå…¥ logits å½¢çŠ¶: (4, 16)
logits (å‰3è¡Œï¼Œå‰5åˆ—):
[[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337]
 [-1.01283112  0.31424733 -0.90802408 -1.4123037   1.46564877]
 [-0.01349722 -1.05771093  0.82254491 -1.22084365  0.2088636 ]]

spatial_logits_matrix = logits @ logits.T
å½¢çŠ¶: (4, 4)
çŸ©é˜µ:
[[14.15461304 -3.47940361 -1.56378682  4.83460684]
 [-3.47940361 14.19068542  4.00727096 -3.84019794]
 [-1.56378682  4.00727096 13.68537383 -0.84682073]
 [ 4.83460684 -3.84019794 -0.84682073 11.02260352]]

æ˜¯å¦å¯¹ç§°: True
ä¾‹å¦‚ [0,1] = -3.4794, [1,0] = -3.4794

å±•å¹³åçš„ spatial_logits: (16,)
å½’ä¸€åŒ–åçš„èŒƒå›´: [-1.0000, 1.0000]

================================================================================
æ­¥éª¤ 2: construct_spatial_connection - æ ¹æ®æ¦‚ç‡é‡‡æ ·è¾¹
================================================================================
æ½œåœ¨è¾¹çš„æ•°é‡: 16
å‰å‡ æ¡è¾¹: [['node_0', 'node_0'], ['node_0', 'node_1'], ['node_0', 'node_2'], ['node_0', 'node_3'], ['node_1', 'node_0']]

spatial_masks (0=ä¸å…è®¸, 1=å…è®¸):
[[0. 1. 1. 1.]
 [1. 0. 1. 1.]
 [1. 1. 0. 1.]
 [1. 1. 1. 0.]]

å¼€å§‹é‡‡æ ·è¾¹...
--------------------------------------------------------------------------------
è¾¹  0: node_0 â†’ node_0 | SKIP (mask=0, è‡ªè¿æ¥)
è¾¹  1: node_0 â†’ node_1 | logit=-0.960 â†’ prob=0.277 | rand=0.696 | âœ— REJECTED
è¾¹  2: node_0 â†’ node_2 | logit=-0.747 â†’ prob=0.321 | rand=0.286 | âœ“ SAMPLED
è¾¹  3: node_0 â†’ node_3 | logit=-0.038 â†’ prob=0.491 | rand=0.227 | âœ“ SAMPLED
è¾¹  4: node_1 â†’ node_0 | logit=-0.960 â†’ prob=0.277 | rand=0.551 | âœ— REJECTED
è¾¹  5: node_1 â†’ node_1 | SKIP (mask=0, è‡ªè¿æ¥)
è¾¹  6: node_1 â†’ node_2 | logit=-0.130 â†’ prob=0.468 | rand=0.719 | âœ— REJECTED
è¾¹  7: node_1 â†’ node_3 | logit=-1.000 â†’ prob=0.269 | rand=0.423 | âœ— REJECTED
è¾¹  8: node_2 â†’ node_0 | logit=-0.747 â†’ prob=0.321 | rand=0.981 | âœ— REJECTED
è¾¹  9: node_2 â†’ node_1 | logit=-0.130 â†’ prob=0.468 | rand=0.685 | âœ— REJECTED
è¾¹ 10: node_2 â†’ node_2 | SKIP (mask=0, è‡ªè¿æ¥)
è¾¹ 11: node_2 â†’ node_3 | logit=-0.668 â†’ prob=0.339 | rand=0.481 | âœ— REJECTED
è¾¹ 12: node_3 â†’ node_0 | logit=-0.038 â†’ prob=0.491 | rand=0.392 | âœ“ SAMPLED
è¾¹ 13: node_3 â†’ node_1 | logit=-1.000 â†’ prob=0.269 | rand=0.343 | âœ— REJECTED
è¾¹ 14: node_3 â†’ node_2 | logit=-0.668 â†’ prob=0.339 | rand=0.729 | âœ— REJECTED
è¾¹ 15: node_3 â†’ node_3 | SKIP (mask=0, è‡ªè¿æ¥)
--------------------------------------------------------------------------------

é‡‡æ ·ç»“æœ:
  æ€»å…±é‡‡æ ·äº† 3 æ¡è¾¹
  æ€» log_prob: -6.3110

é‡‡æ ·çš„è¾¹:
  node_0 â†’ node_2
  node_0 â†’ node_3
  node_3 â†’ node_0

================================================================================
æ­¥éª¤ 3: æ¦‚ç‡åˆ†å¸ƒå¯è§†åŒ–
================================================================================
è¾¹çš„æ¦‚ç‡çŸ©é˜µ (sigmoid(logits)):
[[0.73027117 0.27688212 0.32136657 0.49055519]
 [0.27688212 0.73105858 0.46765711 0.26894142]
 [0.32136657 0.46765711 0.71989694 0.33895105]
 [0.49055519 0.26894142 0.33895105 0.65669351]]

æ¦‚ç‡çŸ©é˜µçš„ç»Ÿè®¡:
  æœ€å°æ¦‚ç‡: 0.2689
  æœ€å¤§æ¦‚ç‡: 0.7311
  å¹³å‡æ¦‚ç‡: 0.4479
  ä¸­ä½æ•°: 0.4033

================================================================================
æ­¥éª¤ 4: éªŒè¯å¯¹ç§°æ€§
================================================================================
å› ä¸º spatial_logits = Z @ Z.T æ˜¯å¯¹ç§°çš„ï¼Œæ‰€ä»¥:
  P(node_0 â†’ node_1) = 0.2769
  P(node_1 â†’ node_0) = 0.2769
  æ˜¯å¦ç›¸ç­‰: True

æ³¨æ„: è™½ç„¶æ¦‚ç‡æ˜¯å¯¹ç§°çš„ï¼Œä½†é‡‡æ ·ç»“æœä¸ä¸€å®šå¯¹ç§°ï¼
ä¾‹å¦‚: å¯èƒ½é‡‡æ ·åˆ° node_0 â†’ node_1ï¼Œä½†æ²¡é‡‡æ ·åˆ° node_1 â†’ node_0

================================================================================
æ­¥éª¤ 5: Temperature çš„å½±å“
================================================================================
Temperature = 0.5: logit=2.0 â†’ prob=0.9820
Temperature = 1.0: logit=2.0 â†’ prob=0.8808
Temperature = 2.0: logit=2.0 â†’ prob=0.7311
Temperature = 5.0: logit=2.0 â†’ prob=0.5987

è§£é‡Š:
  - temperature è¶Šå° â†’ æ¦‚ç‡è¶Šæç«¯ï¼ˆæ¥è¿‘ 0 æˆ– 1ï¼‰
  - temperature è¶Šå¤§ â†’ æ¦‚ç‡è¶Šå¹³æ»‘ï¼ˆæ¥è¿‘ 0.5ï¼‰
  - temperature = 1.0 æ˜¯æ ‡å‡†çš„ sigmoid

================================================================================
æ€»ç»“
================================================================================

1. spatial_logits = Z @ Z.T æ˜¯å¯¹ç§°çŸ©é˜µ
   - æ„å‘³ç€ P(iâ†’j) = P(jâ†’i)
   
2. construct_spatial_connection çš„æµç¨‹:
   a) éå†æ‰€æœ‰æ½œåœ¨çš„è¾¹ (i, j)
   b) æ£€æŸ¥ edge_mask: å¦‚æœä¸º 0ï¼Œè·³è¿‡
   c) è®¡ç®—æ¦‚ç‡: prob = sigmoid(logit / temperature)
   d) éšæœºé‡‡æ ·: if rand() < prob, æ·»åŠ è¿™æ¡è¾¹
   e) è®°å½• log_prob ç”¨äºè®­ç»ƒ
   
3. é‡‡æ ·æ˜¯éšæœºçš„:
   - å³ä½¿ P(iâ†’j) = P(jâ†’i)ï¼Œé‡‡æ ·ç»“æœå¯èƒ½ä¸å¯¹ç§°
   - æ¯æ¬¡è¿è¡Œç»“æœå¯èƒ½ä¸åŒ
   
4. ç”¨äºå¼ºåŒ–å­¦ä¹ :
   - log_probs ç”¨äºè®¡ç®—ç­–ç•¥æ¢¯åº¦
   - é€šè¿‡ REINFORCE ç®—æ³•ä¼˜åŒ–è¾¹çš„é€‰æ‹©
   
5. å…³é”®ä»£ç å¯¹åº”:
   self.spatial_logits = logits @ logits.t()  # å¯¹ç§°çŸ©é˜µ
   edge_prob = torch.sigmoid(edge_logit / temperature)  # è½¬ä¸ºæ¦‚ç‡
   if torch.rand(1) < edge_prob:  # éšæœºé‡‡æ ·
       out_node.add_successor(in_node, 'spatial')  # æ·»åŠ è¾¹



================================================================================
Log Probability è®¡ç®—è¯¦è§£
================================================================================

æ­¥éª¤ 1: éå†æ‰€æœ‰è¾¹ï¼Œè®¡ç®—æ¯æ¡è¾¹çš„ log_prob
--------------------------------------------------------------------------------
è¾¹  0: node_0 â†’ node_0      | SKIP (ä¸è®¡å…¥ log_prob)
è¾¹  1: node_0 â†’ node_1      | âœ— REJECTED  | prob=0.277 â†’ log(1-prob)=-0.3243
è¾¹  2: node_0 â†’ node_2      | âœ“ SAMPLED   | prob=0.321 â†’ log(prob)=-1.1363
è¾¹  3: node_0 â†’ node_3      | âœ“ SAMPLED   | prob=0.491 â†’ log(prob)=-0.7113
è¾¹  4: node_1 â†’ node_0      | âœ— REJECTED  | prob=0.277 â†’ log(1-prob)=-0.3243
è¾¹  5: node_1 â†’ node_1      | SKIP (ä¸è®¡å…¥ log_prob)
è¾¹  6: node_1 â†’ node_2      | âœ— REJECTED  | prob=0.468 â†’ log(1-prob)=-0.6311
è¾¹  7: node_1 â†’ node_3      | âœ— REJECTED  | prob=0.269 â†’ log(1-prob)=-0.3133
è¾¹  8: node_2 â†’ node_0      | âœ— REJECTED  | prob=0.321 â†’ log(1-prob)=-0.3871
è¾¹  9: node_2 â†’ node_1      | âœ— REJECTED  | prob=0.468 â†’ log(1-prob)=-0.6311
è¾¹ 10: node_2 â†’ node_2      | SKIP (ä¸è®¡å…¥ log_prob)
è¾¹ 11: node_2 â†’ node_3      | âœ— REJECTED  | prob=0.339 â†’ log(1-prob)=-0.4140
è¾¹ 12: node_3 â†’ node_0      | âœ“ SAMPLED   | prob=0.491 â†’ log(prob)=-0.7113
è¾¹ 13: node_3 â†’ node_1      | âœ— REJECTED  | prob=0.269 â†’ log(1-prob)=-0.3133
è¾¹ 14: node_3 â†’ node_2      | âœ— REJECTED  | prob=0.339 â†’ log(1-prob)=-0.4140
è¾¹ 15: node_3 â†’ node_3      | SKIP (ä¸è®¡å…¥ log_prob)
--------------------------------------------------------------------------------

æ€» log_prob = sum(log_probs) = -6.3117
log_probs åˆ—è¡¨é•¿åº¦: 13 (åŒ…å«åˆå§‹çš„ 0.0)

================================================================================
å…³é”®ç†è§£
================================================================================

1. ä¸ºä»€ä¹ˆè¦éå†æ‰€æœ‰è¾¹ï¼Ÿ
   - å› ä¸ºè¿™æ˜¯ä¸€ä¸ªæ¦‚ç‡æ¨¡å‹ï¼Œéœ€è¦è®¡ç®—æ•´ä¸ªé‡‡æ ·è¿‡ç¨‹çš„è”åˆæ¦‚ç‡
   - æ¯æ¡è¾¹éƒ½æœ‰ä¸€ä¸ªå†³ç­–ï¼šé‡‡æ · or ä¸é‡‡æ ·
   - æ€»æ¦‚ç‡ = P(è¾¹1çš„å†³ç­–) Ã— P(è¾¹2çš„å†³ç­–) Ã— ... Ã— P(è¾¹Nçš„å†³ç­–)
   - å–å¯¹æ•°åï¼šlog P_total = log P1 + log P2 + ... + log PN

2. ä¸ºä»€ä¹ˆé‡‡æ ·åˆ°çš„è¾¹ç”¨ log(prob)ï¼Œæœªé‡‡æ ·çš„ç”¨ log(1-prob)ï¼Ÿ
   - é‡‡æ ·åˆ°ï¼šè¯´æ˜è¿™æ¬¡éšæœºé‡‡æ ·"æˆåŠŸ"äº†ï¼Œæ¦‚ç‡æ˜¯ prob
   - æœªé‡‡æ ·ï¼šè¯´æ˜è¿™æ¬¡éšæœºé‡‡æ ·"å¤±è´¥"äº†ï¼Œæ¦‚ç‡æ˜¯ 1-prob
   
   ä¾‹å¦‚ï¼š
   - è¾¹çš„æ¦‚ç‡ prob = 0.3
   - å¦‚æœé‡‡æ ·åˆ°ï¼šP(é‡‡æ ·æˆåŠŸ) = 0.3 â†’ log_prob = log(0.3) = -1.204
   - å¦‚æœæœªé‡‡æ ·ï¼šP(é‡‡æ ·å¤±è´¥) = 0.7 â†’ log_prob = log(0.7) = -0.357

3. ä¸ºä»€ä¹ˆè¦è®¡ç®— log_probï¼Ÿ
   - ç”¨äºå¼ºåŒ–å­¦ä¹ çš„ REINFORCE ç®—æ³•
   - æ¢¯åº¦å…¬å¼ï¼šâˆ‡L = -log_prob Ã— reward
   - å¦‚æœ reward é«˜ï¼ˆä»»åŠ¡æˆåŠŸï¼‰ï¼Œå¢åŠ é‡‡æ ·åˆ°çš„è¾¹çš„æ¦‚ç‡
   - å¦‚æœ reward ä½ï¼ˆä»»åŠ¡å¤±è´¥ï¼‰ï¼Œå‡å°‘é‡‡æ ·åˆ°çš„è¾¹çš„æ¦‚ç‡


================================================================================
è¯¦ç»†è®¡ç®—ç¤ºä¾‹
================================================================================

å‡è®¾åªæœ‰ 3 æ¡è¾¹ï¼š
--------------------------------------------------------------------------------
è¾¹çš„ä¿¡æ¯ï¼š
  è¾¹A: prob=0.8, âœ“ é‡‡æ ·åˆ°
  è¾¹B: prob=0.3, âœ— æœªé‡‡æ ·
  è¾¹C: prob=0.6, âœ“ é‡‡æ ·åˆ°

è®¡ç®—è¿‡ç¨‹ï¼š
  è¾¹A: log(0.8) = -0.2231
  è¾¹B: log(1-0.3) = log(0.7) = -0.3567
  è¾¹C: log(0.6) = -0.5108

æ€» log_prob = -1.0906

ç­‰ä»·çš„æ¦‚ç‡è®¡ç®—ï¼ˆéªŒè¯ï¼‰ï¼š
  P(æ€») = 0.8 Ã— (1-0.3) Ã— 0.6 = 0.8 Ã— 0.7 Ã— 0.6 = 0.3360
  log P(æ€») = log(0.3360) = -1.0906
  ä¸ä¸Šé¢çš„ç»“æœä¸€è‡´ï¼

================================================================================
ä»£ç å¯¹åº”
================================================================================

åœ¨ construct_spatial_connection ä¸­ï¼š

```python
log_probs = [torch.tensor(0.0, requires_grad=optimized_spatial)]

for edge_logit, edge_mask in zip(self.spatial_logits, self.spatial_masks):
    if edge_mask == 0.0:
        continue  # è·³è¿‡ï¼Œä¸è®¡å…¥ log_prob
    
    edge_prob = torch.sigmoid(edge_logit / temperature)
    
    if torch.rand(1) < edge_prob:
        # é‡‡æ ·åˆ°è¿™æ¡è¾¹
        out_node.add_successor(in_node, 'spatial')
        log_probs.append(torch.log(edge_prob))  # â† log(prob)
    else:
        # æ²¡é‡‡æ ·åˆ°è¿™æ¡è¾¹
        log_probs.append(torch.log(1 - edge_prob))  # â† log(1-prob)

return torch.sum(torch.stack(log_probs))  # â† æ±‚å’Œ

å…³é”®ç‚¹ï¼š

éå†æ‰€æœ‰å…è®¸çš„è¾¹ï¼ˆedge_mask != 0ï¼‰

æ¯æ¡è¾¹éƒ½è´¡çŒ®ä¸€ä¸ª log_prob

æœ€åæ±‚å’Œå¾—åˆ°æ€» log_prob

ç”¨äºè®¡ç®—ç­–ç•¥æ¢¯åº¦ï¼šloss = -log_prob Ã— utility

================================================================================
ä¸ºä»€ä¹ˆéœ€è¦éå†æ‰€æœ‰è¾¹ï¼Ÿ
è¿™æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„è¦æ±‚ï¼š

ç­–ç•¥ï¼ˆPolicyï¼‰ï¼š

ç­–ç•¥å®šä¹‰äº†åœ¨æ¯ä¸ªçŠ¶æ€ä¸‹é‡‡å–æ¯ä¸ªåŠ¨ä½œçš„æ¦‚ç‡

è¿™é‡Œçš„"åŠ¨ä½œ"æ˜¯"é€‰æ‹©å“ªäº›è¾¹"

ç­–ç•¥çš„æ¦‚ç‡ = P(é€‰æ‹©è¿™ä¸ªå›¾ç»“æ„)

è”åˆæ¦‚ç‡ï¼š

P(å›¾G) = P(è¾¹1) Ã— P(è¾¹2) Ã— ... Ã— P(è¾¹N)

å…¶ä¸­ P(è¾¹i) = prob_i (å¦‚æœé‡‡æ ·) æˆ– (1-prob_i) (å¦‚æœä¸é‡‡æ ·)

å¯¹æ•°æ¦‚ç‡ï¼š

log P(å›¾G) = log P(è¾¹1) + log P(è¾¹2) + ... + log P(è¾¹N)

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¦éå†æ‰€æœ‰è¾¹å¹¶æ±‚å’Œ

æ¢¯åº¦æ›´æ–°ï¼š

âˆ‡ log P(å›¾G) ç”¨äºæ›´æ–°å‚æ•°

å¦‚æœè¿™ä¸ªå›¾ç»“æ„å¯¼è‡´å¥½çš„ç»“æœï¼Œå¢åŠ  P(å›¾G)

å¦‚æœå¯¼è‡´åçš„ç»“æœï¼Œå‡å°‘ P(å›¾G)

æ‰€ä»¥å¿…é¡»éå†æ‰€æœ‰è¾¹ï¼Œæ‰èƒ½æ­£ç¡®è®¡ç®—æ•´ä¸ªå›¾ç»“æ„çš„æ¦‚ç‡ï¼




"""
è¯¦ç»†è§£é‡Š construct_spatial_connection çš„éå†è¿‡ç¨‹
"""

print("=" * 80)
print("construct_spatial_connection éå†è¿‡ç¨‹è¯¦è§£")
print("=" * 80)

print("""
é—®é¢˜ï¼šè¿™é‡Œçš„ loss æ˜¯éå†æ¯ä¸€ä¸ªèŠ‚ç‚¹å—ï¼Ÿæ£€æŸ¥æ¯ä¸€ä¸ªèŠ‚ç‚¹æœ‰æ²¡æœ‰ edgeï¼Ÿ

ç­”æ¡ˆï¼šä¸æ˜¯éå†èŠ‚ç‚¹ï¼Œè€Œæ˜¯éå†æ‰€æœ‰å¯èƒ½çš„è¾¹ï¼ˆedgeï¼‰ï¼
""")

print("\n" + "=" * 80)
print("1. potential_spatial_edges çš„æ„å»º")
print("=" * 80)

print("""
åœ¨ init_potential_edges() ä¸­ï¼š

```python
def init_potential_edges(self):
    for node1_id in self.nodes.keys():
        for node2_id in self.nodes.keys():
            self.potential_spatial_edges.append([node1_id, node2_id])
```

å‡è®¾æœ‰ 4 ä¸ªèŠ‚ç‚¹ï¼šnode_0, node_1, node_2, node_3

potential_spatial_edges åŒ…å«æ‰€æœ‰å¯èƒ½çš„è¾¹ï¼ˆåŒ…æ‹¬è‡ªè¿æ¥ï¼‰ï¼š
  [node_0, node_0]  # 0 â†’ 0 (è‡ªè¿æ¥)
  [node_0, node_1]  # 0 â†’ 1
  [node_0, node_2]  # 0 â†’ 2
  [node_0, node_3]  # 0 â†’ 3
  [node_1, node_0]  # 1 â†’ 0
  [node_1, node_1]  # 1 â†’ 1 (è‡ªè¿æ¥)
  [node_1, node_2]  # 1 â†’ 2
  [node_1, node_3]  # 1 â†’ 3
  [node_2, node_0]  # 2 â†’ 0
  [node_2, node_1]  # 2 â†’ 1
  [node_2, node_2]  # 2 â†’ 2 (è‡ªè¿æ¥)
  [node_2, node_3]  # 2 â†’ 3
  [node_3, node_0]  # 3 â†’ 0
  [node_3, node_1]  # 3 â†’ 1
  [node_3, node_2]  # 3 â†’ 2
  [node_3, node_3]  # 3 â†’ 3 (è‡ªè¿æ¥)

æ€»å…±ï¼š4 Ã— 4 = 16 æ¡å¯èƒ½çš„è¾¹
""")

print("\n" + "=" * 80)
print("2. construct_spatial_connection çš„éå†")
print("=" * 80)

print("""
éå†çš„æ˜¯è¾¹ï¼ˆedgeï¼‰ï¼Œä¸æ˜¯èŠ‚ç‚¹ï¼ˆnodeï¼‰ï¼

```python
for potential_connection, edge_logit, edge_mask in zip(
    self.potential_spatial_edges,  # æ‰€æœ‰å¯èƒ½çš„è¾¹
    self.spatial_logits,            # æ¯æ¡è¾¹çš„ logit
    self.spatial_masks              # æ¯æ¡è¾¹çš„ mask
):
    out_node = self.find_node(potential_connection[0])  # è¾¹çš„èµ·ç‚¹
    in_node = self.find_node(potential_connection[1])   # è¾¹çš„ç»ˆç‚¹
    
    # æ£€æŸ¥è¿™æ¡è¾¹æ˜¯å¦åº”è¯¥è¢«é‡‡æ ·
    ...
```

æ¯æ¬¡å¾ªç¯å¤„ç†ä¸€æ¡è¾¹ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªèŠ‚ç‚¹ï¼
""")

print("\n" + "=" * 80)
print("3. è¯¦ç»†çš„éå†ç¤ºä¾‹")
print("=" * 80)

print("""
å‡è®¾ 4 ä¸ªèŠ‚ç‚¹ï¼Œéå† 16 æ¡è¾¹ï¼š

è¿­ä»£ | è¾¹ (out â†’ in)  | edge_mask | å¤„ç†
-----|----------------|-----------|------
 0   | node_0 â†’ node_0 |    0.0    | SKIP (è‡ªè¿æ¥)
 1   | node_0 â†’ node_1 |    1.0    | é‡‡æ ·å†³ç­–
 2   | node_0 â†’ node_2 |    1.0    | é‡‡æ ·å†³ç­–
 3   | node_0 â†’ node_3 |    1.0    | é‡‡æ ·å†³ç­–
 4   | node_1 â†’ node_0 |    1.0    | é‡‡æ ·å†³ç­–
 5   | node_1 â†’ node_1 |    0.0    | SKIP (è‡ªè¿æ¥)
 6   | node_1 â†’ node_2 |    1.0    | é‡‡æ ·å†³ç­–
 7   | node_1 â†’ node_3 |    1.0    | é‡‡æ ·å†³ç­–
 8   | node_2 â†’ node_0 |    1.0    | é‡‡æ ·å†³ç­–
 9   | node_2 â†’ node_1 |    1.0    | é‡‡æ ·å†³ç­–
10   | node_2 â†’ node_2 |    0.0    | SKIP (è‡ªè¿æ¥)
11   | node_2 â†’ node_3 |    1.0    | é‡‡æ ·å†³ç­–
12   | node_3 â†’ node_0 |    1.0    | é‡‡æ ·å†³ç­–
13   | node_3 â†’ node_1 |    1.0    | é‡‡æ ·å†³ç­–
14   | node_3 â†’ node_2 |    1.0    | é‡‡æ ·å†³ç­–
15   | node_3 â†’ node_3 |    0.0    | SKIP (è‡ªè¿æ¥)

æ¯æ¡è¾¹éƒ½ä¼šè¢«æ£€æŸ¥ä¸€æ¬¡ï¼
""")

print("\n" + "=" * 80)
print("4. æ¯æ¡è¾¹çš„å¤„ç†é€»è¾‘")
print("=" * 80)

print("""
å¯¹äºæ¯æ¡è¾¹ (out_node â†’ in_node)ï¼š

æ­¥éª¤ 1: æ£€æŸ¥ edge_mask
  if edge_mask == 0.0:
      continue  # è·³è¿‡è¿™æ¡è¾¹ï¼ˆå¦‚è‡ªè¿æ¥ï¼‰

æ­¥éª¤ 2: æ£€æŸ¥æ˜¯å¦ä¼˜åŒ–
  elif edge_mask == 1.0 and self.optimized_spatial == False:
      # ä¸ä¼˜åŒ–æ¨¡å¼ï¼šç›´æ¥æ·»åŠ è¾¹
      out_node.add_successor(in_node, 'spatial')
      continue

æ­¥éª¤ 3: é‡‡æ ·å†³ç­–ï¼ˆä¼˜åŒ–æ¨¡å¼ï¼‰
  edge_prob = torch.sigmoid(edge_logit / temperature)
  
  if torch.rand(1) < edge_prob:
      # é‡‡æ ·æˆåŠŸï¼šæ·»åŠ è¿™æ¡è¾¹
      out_node.add_successor(in_node, 'spatial')
      log_probs.append(torch.log(edge_prob))
  else:
      # é‡‡æ ·å¤±è´¥ï¼šä¸æ·»åŠ è¿™æ¡è¾¹
      log_probs.append(torch.log(1 - edge_prob))
""")

print("\n" + "=" * 80)
print("5. log_probs çš„ç´¯ç§¯")
print("=" * 80)

print("""
log_probs è®°å½•äº†æ‰€æœ‰è¾¹çš„é‡‡æ ·æ¦‚ç‡ï¼š

åˆå§‹ï¼šlog_probs = [0.0]

éå†è¾¹ï¼š
  è¾¹ 0 (0â†’0): SKIP (ä¸æ·»åŠ åˆ° log_probs)
  è¾¹ 1 (0â†’1): æœªé‡‡æ · â†’ log_probs.append(log(1-0.277)) = -0.324
  è¾¹ 2 (0â†’2): é‡‡æ ·åˆ° â†’ log_probs.append(log(0.321)) = -1.136
  è¾¹ 3 (0â†’3): é‡‡æ ·åˆ° â†’ log_probs.append(log(0.491)) = -0.711
  è¾¹ 4 (1â†’0): æœªé‡‡æ · â†’ log_probs.append(log(1-0.277)) = -0.324
  è¾¹ 5 (1â†’1): SKIP
  è¾¹ 6 (1â†’2): æœªé‡‡æ · â†’ log_probs.append(log(1-0.468)) = -0.631
  ...

æœ€åï¼šreturn torch.sum(torch.stack(log_probs))
     = 0.0 + (-0.324) + (-1.136) + (-0.711) + ... = -6.31
""")

print("\n" + "=" * 80)
print("6. å…³é”®ç†è§£")
print("=" * 80)

print("""
âœ“ éå†çš„æ˜¯è¾¹ï¼ˆedgeï¼‰ï¼Œä¸æ˜¯èŠ‚ç‚¹ï¼ˆnodeï¼‰
  - 4 ä¸ªèŠ‚ç‚¹ â†’ 16 æ¡å¯èƒ½çš„è¾¹ï¼ˆ4Ã—4ï¼‰
  - N ä¸ªèŠ‚ç‚¹ â†’ NÂ² æ¡å¯èƒ½çš„è¾¹

âœ“ æ¯æ¡è¾¹éƒ½è¦åšå†³ç­–
  - é‡‡æ ·åˆ°ï¼šæ·»åŠ è¾¹ï¼Œè®°å½• log(prob)
  - æœªé‡‡æ ·ï¼šä¸æ·»åŠ è¾¹ï¼Œè®°å½• log(1-prob)
  - è·³è¿‡ï¼šmask=0ï¼Œä¸è®°å½•

âœ“ log_probs æ˜¯æ‰€æœ‰è¾¹çš„è”åˆæ¦‚ç‡
  - è¡¨ç¤º"é€‰æ‹©è¿™ä¸ªå›¾ç»“æ„"çš„æ€»æ¦‚ç‡
  - ç”¨äº REINFORCE ç®—æ³•çš„æ¢¯åº¦è®¡ç®—

âœ“ æ¯ä¸ªèŠ‚ç‚¹å¯èƒ½æœ‰å¤šæ¡å‡ºè¾¹
  - node_0 å¯èƒ½è¿åˆ° node_1, node_2, node_3
  - æ¯æ¡è¾¹ç‹¬ç«‹é‡‡æ ·
""")

print("\n" + "=" * 80)
print("7. å¯è§†åŒ–ç¤ºä¾‹")
print("=" * 80)

print("""
å‡è®¾é‡‡æ ·ç»“æœï¼š

èŠ‚ç‚¹å…³ç³»ï¼š
  node_0 â†’ node_2 âœ“ (é‡‡æ ·åˆ°)
  node_0 â†’ node_3 âœ“ (é‡‡æ ·åˆ°)
  node_1 â†’ node_2 âœ— (æœªé‡‡æ ·)
  node_2 â†’ node_3 âœ— (æœªé‡‡æ ·)
  node_3 â†’ node_0 âœ“ (é‡‡æ ·åˆ°)

å›¾ç»“æ„ï¼š
    â”Œâ”€â”€â”€â”€â”€â”
    â”‚  0  â”‚â”€â”€â”
    â””â”€â”€â–²â”€â”€â”˜  â”‚
       â”‚     â”‚
       â”‚     â–¼
    â”Œâ”€â”€â”´â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
    â”‚  3  â”‚ â”‚  2  â”‚
    â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
       â–²       â–²
       â”‚       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”˜
         (from 0)

æ¯æ¡è¾¹éƒ½è¢«æ£€æŸ¥äº†ï¼Œä½†åªæœ‰éƒ¨åˆ†è¢«é‡‡æ ·åˆ°ï¼
""")

print("\n" + "=" * 80)
print("æ€»ç»“")
print("=" * 80)

print("""
é—®ï¼šéå†æ¯ä¸€ä¸ªèŠ‚ç‚¹å—ï¼Ÿ
ç­”ï¼šä¸æ˜¯ï¼éå†çš„æ˜¯æ¯ä¸€æ¡å¯èƒ½çš„è¾¹ï¼ˆedgeï¼‰

é—®ï¼šæ£€æŸ¥æ¯ä¸€ä¸ªèŠ‚ç‚¹æœ‰æ²¡æœ‰ edgeï¼Ÿ
ç­”ï¼šä¸æ˜¯ï¼æ£€æŸ¥çš„æ˜¯æ¯ä¸€æ¡è¾¹æ˜¯å¦åº”è¯¥è¢«é‡‡æ ·

å…³é”®ç‚¹ï¼š
1. éå†å¯¹è±¡ï¼šè¾¹ï¼ˆedgeï¼‰ï¼Œä¸æ˜¯èŠ‚ç‚¹ï¼ˆnodeï¼‰
2. éå†æ•°é‡ï¼šNÂ² æ¡è¾¹ï¼ˆN æ˜¯èŠ‚ç‚¹æ•°ï¼‰
3. æ¯æ¡è¾¹ï¼šç‹¬ç«‹é‡‡æ ·ï¼Œç‹¬ç«‹è´¡çŒ® log_prob
4. æœ€ç»ˆï¼šæ‰€æœ‰è¾¹çš„ log_prob æ±‚å’Œ
""")




# 1. edge_logit æ¥è‡ª spatial_logits
spatial_logits_matrix = logits @ logits.t()  # [4, 4] å¯¹ç§°çŸ©é˜µ
# ä¾‹å¦‚ï¼š
# [[a, b, c, d],
#  [b, e, f, g],
#  [c, f, h, i],
#  [d, g, i, j]]

self.spatial_logits = flatten(spatial_logits_matrix)  # [16]
# [a, b, c, d, b, e, f, g, c, f, h, i, d, g, i, j]

# åœ¨ construct_spatial_connection ä¸­:
for edge_logit in self.spatial_logits:
    # edge_logit å°±æ˜¯æ¯æ¡è¾¹çš„ logit å€¼
    edge_prob = sigmoid(edge_logit)
    # é‡‡æ ·...

# 2. optimized_spatial é»˜è®¤å€¼
# Graph.__init__: optimized_spatial = False (é»˜è®¤)
# run_gpqa_cache_API.py: --optimized_spatial default=True (ä½ çš„è®¾ç½®)

# æ‰€ä»¥åœ¨ä½ çš„å®éªŒä¸­ï¼Œoptimized_spatial = True
# è¿™æ„å‘³ç€ï¼š
# - spatial_logits çš„ requires_grad = True
# - ä¼šé€šè¿‡æ¢¯åº¦ä¸‹é™ä¼˜åŒ–è¾¹çš„é€‰æ‹©
# - ç”¨äºå­¦ä¹ æœ€ä¼˜çš„å›¾ç»“æ„