# CacheDesigner: Graph-Guided Cache Communication for Multi-Agent LLMs

Combines **GDesigner's** graph topology learning with **LatentMAS's** KV-cache communication.

---

## ğŸš€ Quick Start (Use Your Free API)

```bash
cd G-cache/experiments

# Run with cache (recommended)
python run_gsm8k_cache_API.py --use_cache --optimized_spatial --batch_size 2

# Run without cache (baseline)
python run_gsm8k_cache_API.py --optimized_spatial --batch_size 2
```

**Uses your free Qwen API - no GPU needed!**

---

## ğŸ“ Project Structure

```
G-cache/
â”œâ”€â”€ GDesigner/                      # GDesigner backbone
â”‚   â”œâ”€â”€ graph/
â”‚   â”‚   â”œâ”€â”€ graph.py               # Original Graph
â”‚   â”‚   â””â”€â”€ cache_graph.py         # NEW: Cache-enabled Graph
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ gpt_chat.py            # Original API LLM
â”‚   â”‚   â””â”€â”€ gpt_chat_cache_api.py  # NEW: Cache-enabled API LLM
â”‚   â””â”€â”€ agents/
â”‚       â”œâ”€â”€ math_solver.py         # Original agent
â”‚       â””â”€â”€ math_solver_cache.py   # NEW: Cache-enabled agent
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_gsm8k.py               # Original runner
â”‚   â”œâ”€â”€ run_gsm8k_cache_API.py     # NEW: API + cache (use this!)
â”‚   â””â”€â”€ run_gsm8k_cache_WORKING.py # NEW: vLLM + cache (needs GPU)
â””â”€â”€ .env                            # Your API credentials
```

---

## ğŸ¯ Two Versions

### Option 1: API Version (FREE - Recommended) âœ…

**What it uses:**
- Your free Qwen API for text generation
- Simulated cache for testing structure

**Pros:**
- âœ… FREE (uses your API)
- âœ… No GPU needed
- âœ… Tests full structure
- âœ… Easy to run

**Cons:**
- âš ï¸ Cache is simulated (not real KV-cache)
- âš ï¸ ~0-5% improvement

**Run:**
```bash
python run_gsm8k_cache_API.py --use_cache --optimized_spatial
```

### Option 2: vLLM Version (Needs GPU) 

**What it uses:**
- Local model with vLLM
- Real KV-cache extraction

**Pros:**
- âœ… Real cache benefits
- âœ… ~10-25% improvement

**Cons:**
- âŒ Needs GPU (16GB+ VRAM)
- âŒ Costs money ($1-2/hour)

**Run:**
```bash
python run_gsm8k_cache_WORKING.py --use_cache --device cuda
```

---

## ğŸ” Debug Output

When running, you'll see:

```
ğŸ¤– [AGENT abc1] Executing...
   ğŸ” Checking for predecessor caches...
   âœ… Found fused cache from predecessors

ğŸŒ [API CALL] Calling Qwen API...
   Status: 200
   âœ… SUCCESS: Received 245 characters

ğŸ“¦ [CACHE] agen_with_cache called
   ğŸ”„ Using cached context from 32 layers
   âœ… Cache generated: 32 layers

ğŸ’¾ [GRAPH] Storing cache for node abc1
   Cache layers: 32

ğŸ”„ [GRAPH] Getting fused cache for node xyz2
   âœ… Found cache from abc1
   ğŸ§ª Fusing 2 caches
```

**Success indicators:**
1. âœ… API calls succeed
2. âœ… Cache generated
3. âœ… Cache stored
4. âœ… Cache retrieved
5. âœ… Cache fused
6. âœ… Cache used

---

## ğŸ”§ Setup

### 1. Check .env file exists:
```bash
cat .env
# Should show:
# BASE_URL=https://idealab-external.alibaba-inc.com/api/openai/v1
# API_KEY=c3a588a3e15983ab2dc8facefecc5bd9
```

### 2. Install dependencies:
```bash
pip install torch transformers aiohttp python-dotenv tenacity
```

### 3. Run:
```bash
cd experiments
python run_gsm8k_cache_API.py --use_cache --optimized_spatial
```

---

## ğŸ“Š What's Different from GDesigner?

| Aspect | GDesigner | CacheDesigner |
|--------|-----------|---------------|
| **Communication** | Text only | Text + Cache |
| **Agent class** | `MathSolver` | `MathSolverCache` |
| **LLM class** | `GPTChat` | `GPTChatCacheAPI` |
| **Graph class** | `Graph` | `CacheGraph` |
| **Cache extraction** | âŒ No | âœ… Yes |
| **Cache fusion** | âŒ No | âœ… Yes |
| **Cache injection** | âŒ No | âœ… Yes |

---

## ğŸ› Troubleshooting

### Error: "403 Client Error"
```bash
# Check .env file
cat .env
# Make sure BASE_URL and API_KEY are set correctly
```

### Error: "No module named 'GDesigner'"
```bash
# Make sure you're in the right directory
cd /Users/bleachvex/Downloads/projects/G-cache/experiments
```

### No cache operations shown
```bash
# Make sure --use_cache flag is set
python run_gsm8k_cache_API.py --use_cache  # â† Must have this!
```

---

## ğŸ“ Key Files

**To run experiments:**
- `experiments/run_gsm8k_cache_API.py` - Main runner (use this!)

**Core implementation:**
- `GDesigner/llm/gpt_chat_cache_api.py` - Cache-enabled LLM
- `GDesigner/agents/math_solver_cache.py` - Cache-enabled agent
- `GDesigner/graph/cache_graph.py` - Cache-enabled graph

**Configuration:**
- `.env` - Your API credentials

---

## ğŸ¯ Summary

**What is CacheDesigner?**
- GDesigner (graph topology) + LatentMAS (cache communication)

**Which version should I use?**
- API version (free, simulated cache)

**Do I need GPU?**
- No (for API version)

**Does it use my API?**
- Yes (same as G-Designer)

**How do I run it?**
```bash
cd experiments
python run_gsm8k_cache_API.py --use_cache --optimized_spatial
```

**That's it!** ğŸ‰
