# G-cache: Graph-Guided KV-Cache Communication for Multi-Agent LLMs

Combines **GDesigner's** graph topology with **LatentMAS's** KV-cache generation.

---

## ğŸš€ Quick Start

```bash
cd experiments

# Hybrid mode (RECOMMENDED) - small GPU + free API
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache
```

---

## ğŸ“ What Gets Updated During Training?

### Critical Insight: LatentMAS vs G-cache

**LatentMAS (original):**
- âŒ **NO training** - Inference only!
- âŒ No optimizer, no loss.backward()
- âŒ All models frozen
- Just runs multi-agent inference with cache

**G-cache (this project):**
- âœ… **HAS training** - Learns graph structure!
- âœ… Optimizer updates GCN + CacheFuser
- âœ… Learns from task performance

### Models in G-cache

| Model | Trainable? | Updated? | Purpose | LatentMAS Has This? |
|-------|-----------|----------|---------|---------------------|
| **1. GCN** | âœ… Yes | âœ… Yes | Graph edge weights | âŒ No (NEW in G-cache) |
| **2. CacheFuser** | âœ… Yes | âœ… Yes | Cache fusion weights | âŒ No (NEW in G-cache) |
| **3. Small Local Model** | âŒ Frozen | âŒ No | Cache generation | âœ… Yes (frozen in both) |
| **4. API Model** | âŒ External | âŒ No | Text generation | âœ… Yes (frozen in both) |

### Training Code (G-cache ONLY)

```python
# File: experiments/run_gsm8k_cache_API.py

# Step 1: Setup optimizer with trainable components (NEW in G-cache)
params = list(graph.gcn.parameters())  # â† GCN weights (NEW)
if args.use_cache:
    params += list(graph.cache_fuser.parameters())  # â† CacheFuser weights (NEW)
optimizer = torch.optim.Adam(params, lr=0.1)

# Step 2: Forward pass
answer, log_prob = await graph.arun(question)

# Step 3: Compute loss (NEW in G-cache)
is_correct = (answer == gold_answer)
utility = float(is_correct)
loss = -log_prob * utility

# Step 4: Backprop (NEW in G-cache)
optimizer.zero_grad()
loss.backward()  # Updates:
                 # âœ… GCN.edge_weights (NEW)
                 # âœ… CacheFuser.layer_gates (NEW)
                 # âœ… CacheFuser.fusion_weights (NEW)
                 # âŒ Small local model (frozen - same as LatentMAS)
                 # âŒ API model (external - same as LatentMAS)
optimizer.step()
```

### What LatentMAS Does (Inference Only)

```python
# File: LatentMAS/run.py

# NO optimizer!
# NO loss!
# NO training!

# Just inference:
for item in dataset:
    result = method.run_batch([item])  # Generate answer
    is_correct = (result['prediction'] == item['gold'])
    # That's it - no backprop!
```

### Comparison

| Aspect | LatentMAS | G-cache |
|--------|-----------|---------|
| **Training** | âŒ No | âœ… Yes |
| **Optimizer** | âŒ No | âœ… Yes (Adam) |
| **Loss** | âŒ No | âœ… Yes (-log_prob * utility) |
| **Backprop** | âŒ No | âœ… Yes |
| **Trainable params** | 0 | GCN + CacheFuser |
| **Cache generation** | âœ… Yes (frozen model) | âœ… Yes (frozen model) |
| **Graph structure** | Fixed (sequential/hierarchical) | âœ… Learned |
| **Cache fusion** | Fixed (concatenation) | âœ… Learned |

### Guarantee: No Missing Updates

**LatentMAS updates**: 0 models (inference only)

**G-cache updates**: 2 models
1. âœ… GCN - Added to optimizer (line 108)
2. âœ… CacheFuser - Added to optimizer (line 110)

**Verification:**
```python
# Check what's in optimizer
params = list(graph.gcn.parameters())  # â† All GCN params
params += list(graph.cache_fuser.parameters())  # â† All CacheFuser params
optimizer = torch.optim.Adam(params, lr=0.1)

# These are ALL trainable parameters in the system!
# Small model and API are explicitly frozen/external
```

**Guarantee**: âœ… We update everything that should be updated!

---

## ğŸ”— How Graph Functions Connect with Cache Functions

### The Connection Chain

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. CacheGraph (cache_graph.py)                                 â”‚
â”‚    - Manages cache storage: node_caches = {}                   â”‚
â”‚    - Provides: get_fused_cache(), store_node_cache()           â”‚
â”‚    - Trainable: CacheFuser âœ…                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ passes graph reference
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. MathSolverCache (math_solver_cache.py)                      â”‚
â”‚    - Receives: self.graph = graph                              â”‚
â”‚    - Calls: graph.get_fused_cache(self)                        â”‚
â”‚    - Calls: graph.store_node_cache(self.id, cache)             â”‚
â”‚    - Trainable: None                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ calls LLM
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. HybridCacheLLM (llm_cache_hybrid.py)                        â”‚
â”‚    - Receives: past_key_values from graph                      â”‚
â”‚    - Calls: hybrid_model.generate_latent_batch()               â”‚
â”‚    - Returns: (text, kv_cache) to agent                        â”‚
â”‚    - Trainable: None                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ calls LatentMAS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. HybridCacheModel (hybrid_cache_model.py)                    â”‚
â”‚    - EXACT LatentMAS implementation                            â”‚
â”‚    - generate_latent_batch(past_key_values=fused_cache)        â”‚
â”‚    - Returns: new_cache                                        â”‚
â”‚    - Trainable: None (frozen) âŒ                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Connections Summary

| Connection | From | To | Data | Function |
|------------|------|----|----|----------|
| 1 | CacheGraph | Agent | `graph` reference | `node.graph = self` |
| 2 | Agent | CacheGraph | `self` | `graph.get_fused_cache(self)` |
| 3 | CacheGraph | Agent | `fused_cache` | Returns fused cache |
| 4 | Agent | LLM | `past_key_values` | `llm.agen_with_cache(..., past_key_values)` |
| 5 | LLM | LatentMAS | `past_key_values` | `generate_latent_batch(..., past_key_values)` |
| 6 | LatentMAS | LLM | `new_cache` | Returns cache |
| 7 | LLM | Agent | `(text, cache)` | Returns tuple |
| 8 | Agent | CacheGraph | `cache` | `graph.store_node_cache(id, cache)` |

---

## ğŸ¯ Four Modes

### Mode 1: Pure Local with Real Cache (BEST for Research) âœ…

```bash
cd experiments
export CUDA_VISIBLE_DEVICES=1,2  # Use 2 GPUs for 7B model
python run_gsm8k_cache.py --llm_name Qwen/Qwen3-8B --use_cache --device cuda
```

**What it does**:
- âœ… **Real KV-cache** from local model (LatentMAS)
- âœ… **Alignment matrix** enabled by default
- âœ… Trains GCN + CacheFuser
- âœ… No API needed (fully local)
- âœ… Uses HuggingFace transformers (no vLLM)

**GPU**: ~16GB for 8B model (or ~4GB for 1.7B model)

**Model options**:
- `Qwen/Qwen3-1.7B` - Small, ~4GB GPU
- `Qwen/Qwen3-4B` - Medium, ~8GB GPU  
- `Qwen/Qwen3-8B` - Large, ~16GB GPU (recommended)

**Use this for**: Real cache experiments, full control

---

### Mode 2: Hybrid (Small GPU + API) âœ…

```bash
cd experiments
export CUDA_VISIBLE_DEVICES=1  # Only need 1 GPU
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache
```

**What it does**:
- âœ… Small local model (1.5B) generates **real KV-cache**
- âœ… API (qwen-flash) generates final text (faster & cheaper than qwen-plus)
- âœ… **Alignment matrix** enabled
- âš ï¸ Requires API key

**GPU**: ~4GB only

**API models available**: `qwen-flash` (default), `qwen-plus`, `qwen-turbo`

**Use this for**: When you have API access

---

### Mode 3: API Baseline (No Cache)

```bash
cd experiments
python run_gsm8k_cache_API.py --llm_name qwen-flash  # or qwen-plus, qwen-turbo
```

**What it does**:
- âŒ No real cache (text-based only)
- Uses API for everything
- Baseline for comparison

**GPU**: 0GB

---

### Mode 4: Local Cache (Deprecated)

```bash
cd experiments
export CUDA_VISIBLE_DEVICES=1,2
python run_gsm8k_cache_API.py --llm_name local_cache --use_cache
```

**Note**: Use Mode 1 (`run_gsm8k_cache.py`) instead - it's the same but better tested

---

## ğŸ”§ Setup

```bash
# 1. Install dependencies
pip install torch transformers openai python-dotenv

# 2. Set API key in .env file (already done!)
# Just edit .env:
# API_KEY=your_dashscope_api_key

# 3. Run (no export needed!)
cd experiments
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache
```

**Note**: The code automatically loads `API_KEY` from `.env` - no need to export anything!

---

## ğŸ”¬ LatentMAS Alignment Matrix (Training-Free)

### What is it?

LatentMAS uses a **projection matrix** `W_a` to align hidden states back to valid input embeddings:

```
e = h * W_a, where W_a â‰ˆ W_out^(-1) * W_in
```

**Problem**: Hidden states from last layer have different distribution than input embeddings

**Solution**: Linear transformation that maps output space â†’ input space (training-free!)

### Implementation in LatentMAS

**File**: `LatentMAS/models.py`

```python
class ModelWrapper:
    def _build_latent_realign_matrix(self, model, device, args):
        """Compute W_a = (W_out^T * W_out)^(-1) * W_out^T * W_in"""
        input_weight = model.get_input_embeddings().weight   # W_in
        output_weight = model.get_output_embeddings().weight # W_out
        
        # Solve: W_out * W_a = W_in
        gram = torch.matmul(output_weight.T, output_weight)  # W_out^T * W_out
        reg = 1e-5 * torch.eye(gram.shape[0])                # Regularization
        gram = gram + reg
        rhs = torch.matmul(output_weight.T, input_weight)    # W_out^T * W_in
        realign_matrix = torch.linalg.solve(gram, rhs)       # W_a
        
        target_norm = input_weight.norm(dim=1).mean()        # For normalization
        return realign_matrix, target_norm
    
    def _apply_latent_realignment(self, hidden, model):
        """Apply: e = normalize(h * W_a)"""
        matrix, target_norm = self._ensure_latent_realign_matrix(model, hidden.device)
        aligned = torch.matmul(hidden.float(), matrix)       # h * W_a
        
        # Normalize to match input embedding norms
        aligned_norm = aligned.norm(dim=-1, keepdim=True).clamp_min(1e-6)
        aligned = aligned * (target_norm / aligned_norm)
        return aligned.to(hidden.dtype)
    
    def generate_latent_batch(self, ...):
        for step in range(latent_steps):
            # KEY: Apply alignment before feeding back
            latent_vec = self._apply_latent_realignment(last_hidden, self.model)
            latent_embed = latent_vec.unsqueeze(1)
            
            outputs = self.model(
                inputs_embeds=latent_embed,  # â† Aligned embedding
                ...
            )
```

**Usage**: `python run.py --latent_space_realign` (optional flag)

### Does G-cache Have This?

**Answer**: âœ… **YES! Now implemented!**

**File**: `G-cache/hybrid_cache_model.py`

```python
class HybridCacheModel:
    def __init__(self, ..., use_alignment: bool = True):
        # Build alignment matrix (training-free, computed once)
        if self.use_alignment:
            self._build_alignment_matrix()
    
    def _build_alignment_matrix(self):
        """Build W_a â‰ˆ W_out^(-1) * W_in"""
        W_in = self.cache_model.get_input_embeddings().weight
        W_out = self.cache_model.get_output_embeddings().weight
        gram = torch.matmul(W_out.T, W_out) + 1e-5 * I
        rhs = torch.matmul(W_out.T, W_in)
        self._alignment_matrix = torch.linalg.solve(gram, rhs)
    
    def _apply_alignment(self, hidden):
        """Apply: e = normalize(h * W_a)"""
        aligned = torch.matmul(hidden, self._alignment_matrix)
        return aligned * (target_norm / ||aligned||)
    
    def generate_latent_batch(self, ...):
        for step in range(latent_steps):
            latent_vec = self._apply_alignment(last_hidden)  # âœ… Now aligned!
            latent_embed = latent_vec.unsqueeze(1)
```

**Usage**: Enabled by default! Disable with `use_alignment=False`

### Comparison

| Aspect | LatentMAS | G-cache |
|--------|-----------|---------||
| **Alignment matrix** | âœ… Optional (`--latent_space_realign`) | âœ… **Enabled by default!** |
| **Matrix computation** | `W_a â‰ˆ W_out^(-1) * W_in` | âœ… Same |
| **Normalization** | âœ… Match input embedding norms | âœ… Same |
| **Training-free** | âœ… Yes (computed once) | âœ… Yes (computed once) |
| **Implementation** | 3 methods in ModelWrapper | âœ… 2 methods in HybridCacheModel |

### Benefits in G-cache

**âœ… Enabled by default**:
- Better cache quality (aligned embeddings)
- Training-free (computed once at init)
- Minimal overhead (one matrix multiply per latent step)

**Disable if needed**:
```python
model = HybridCacheModel(use_alignment=False)  # Disable alignment
```

---

## ğŸ¯ Summary

**What**: Graph manages cache flow, LatentMAS generates cache

**LatentMAS**: Inference only (no training) + optional alignment matrix

**G-cache**: Trains GCN + CacheFuser (learns graph structure + fusion)

**Frozen**: Small local model + API model (same as LatentMAS)

**Alignment**: Both have it! (LatentMAS optional, G-cache default)

**Run (Pure Local - RECOMMENDED)**:
```bash
cd experiments
export CUDA_VISIBLE_DEVICES=1,2
python run_gsm8k_cache.py --llm_name Qwen/Qwen3-1.7B --use_cache --device cuda
```

**That's it!** ğŸ‰

---

## ğŸ”„ Cache Flow Explained

### What Are Agent IDs?

Agent IDs like `3KsM`, `QXan`, `5Hp5` are randomly generated 4-character identifiers for each agent node in the graph.

### Agent Graph Structure

```
     3KsM (MathSolver) â†’ cache: 2377 tokens
     QXan (MathSolver) â†’ cache: 474 tokens  
     5Hp5 (MathSolver) â†’ cache: 181 tokens
       â†“ â†“ â†“ (all feed into)
     5xcm (Decision Maker)
       â†“ receives: 3KsM's cache (first one)
       â†“ generates: final answer
```

### What Comes From Cache vs LLM?

| Component | What It Does | Visible? | Example |
|-----------|-------------|----------|----------|
| **Cache (GPU)** | Stores reasoning as tensors | âŒ No | `torch.Size([1, 2, 2377, 128])` |
| **API (qwen-flash)** | Generates text responses | âœ… Yes | "Let's solve step by step..." |
| **Cache Fusion** | Combines multiple caches | âŒ No | Internal process |
| **Graph Structure** | Routes cache between agents | âœ… Yes | Shown in logs |

**Key Insight**: Cache = "compressed thoughts" that agents pass to each other. API converts these thoughts into readable text.

### âš ï¸ Important: How Cache is Used

**Cache is NOT directly used by API!** Here's the actual flow:

1. **Small local model (GPU)** generates KV-cache tensors
2. **Cache is converted to text context**: `"[Using 28 layers of cached reasoning from previous agents, 2377 tokens]"`
3. **This text is prepended to the API prompt**
4. **API (qwen-flash) generates response** based on the text context (NOT the raw cache)

**Why?** APIs don't support KV-cache input - they only accept text. The cache serves as a "hint" to the API about what previous agents computed.

### Cache Flow Example

**Without Cache (Cold Start)**:
```
Agent 3KsM:
  ğŸ†• No input cache
  ğŸ§  Generates from scratch
  ğŸ“ Output: "Let's solve... Total = $64"
  ğŸ’¾ Stores cache: 2377 tokens
```

**With Cache (Warm Start)**:
```
Agent 5xcm:
  ğŸ”— Receives cache from 3KsM (2377 tokens)
  ğŸ§  Continues from 3KsM's reasoning
  ğŸ“ Output: "After reviewing... The answer is 64"
  ğŸ’¾ Stores cache: 3724 tokens (grew from 2377)
```

### Sequence Length Mismatch

**Problem**: Can't fuse caches of different lengths (2377 â‰  474 â‰  181)

**Solution (LatentMAS-proven)**: Use first cache only
- Takes 3KsM's cache (2377 tokens)
- Ignores QXan and 5Hp5 caches
- Sequential passing (proven by LatentMAS paper)

### Debug Markers

Look for these in output:
```
ğŸ”— [CACHE] Using input cache with 28 layers     â† Cache is being used
ğŸ†• [CACHE] No input cache - generating from scratch  â† No cache (first agent)
âœ… [CACHE] Generated cache with 28 layers, seq_len=2377  â† Cache created
ğŸ¤– [API] Calling qwen-flash to generate text...  â† API generates text
ğŸ“ [API] Generated 1234 characters of text       â† Text output
```

### Cache Structure Details

**What is `past_key_values`?**

Type: Tuple of tuples (NOT text, NOT string)

```python
past_key_values = (
    (key_tensor_0, value_tensor_0),    # Layer 0: [1, 2, 2377, 128]
    (key_tensor_1, value_tensor_1),    # Layer 1: [1, 2, 2377, 128]
    # ... 26 more layers ...
    (key_tensor_27, value_tensor_27)   # Layer 27: [1, 2, 2377, 128]
)
```

**Total**: 28 layers Ã— 2 tensors (key + value) = 56 tensors

**Tensor shape**: `[batch=1, heads=2, seq_len=2377, hidden_dim=128]`
- Data type: `torch.bfloat16` (16-bit floating point)
- Device: `cuda:0` (GPU memory)

**Why Can't API Use This?**

API endpoints only accept TEXT, not binary tensors.

**Solution**: Convert to text hint
```python
# Cache (tensor): 56 tensors Ã— millions of numbers
past_key_values = ((tensor1, tensor2), ...)

# Converted to text:
cache_info = "[Using 28 layers of cached reasoning, 2377 tokens]"

# Sent to API:
prompt = cache_info + "\n\n" + original_prompt
```

**Analogy**: Cache = full book (tensor data), API receives = "This person read a 300-page book" (text hint)

### Two Ways to Generate Text

**Method 1: Local Model with Cache Tensors (REAL cache usage)**
```python
# Function: generate_text_batch()
# Uses: Local GPU model (Qwen2.5-1.5B)
# Cache: Passed DIRECTLY as tensors to model.generate()

outputs = model.generate(
    input_ids=input_ids,
    past_key_values=cache_tensors,  # â† REAL cache usage!
    ...
)
```

**Method 2: API with Text Hint (Simulated cache)**
```python
# Function: generate_text_batch_api()
# Uses: API (qwen-flash)
# Cache: Converted to text hint, prepended to prompt

prompt = "[Using 28 layers of cache, 2377 tokens]\n\n" + original_prompt
response = api.chat.completions.create(
    messages=[{"content": prompt}],  # â† Text hint only
    ...
)
```

**Comparison**:

| Aspect | Local (Real Cache) | API (Text Hint) |
|--------|-------------------|------------------|
| **Cache Format** | Tensor objects | Text string |
| **Cache Usage** | Direct (model.generate) | Indirect (text context) |
| **GPU Required** | Yes (~4GB) | No |
| **Speed** | Faster (cache reuse) | Slower (no cache reuse) |
| **Quality** | Good (small model) | Better (large API model) |
| **Cost** | GPU cost | API cost |
| **Best For** | Pure local | API only | **Production** â­ |

**Key Difference**:
- **Local**: `model.generate(past_key_values=cache_tensors)` â† Tensors used directly
- **API**: `api.create(messages=[text_hint])` â† Only text hint sent

**Why API Can't Use Cache Tensors**:
- API endpoints only accept JSON/text
- Can't send binary tensor data over HTTP
- Solution: Convert cache metadata to text hint

**Method 3: TRUE Hybrid (BEST - combines both!)**
```python
# Function: generate_text_batch_hybrid()
# Step 1: Local model uses cache tensors (fast)
# Step 2: API refines output (high quality)

local_text, new_cache = model.generate_text_batch(
    input_ids, past_key_values=cache_tensors  # â† Real cache!
)
api_text, _ = await api.generate_text_batch_api(
    messages_with_local_context  # â† API refines
)
return api_text, new_cache  # Best of both!
```

**Trade-off**:
- Local only: Real cache âœ… but small model âŒ
- API only: Large model âœ… but no cache âŒ
- **Hybrid**: Real cache âœ… + Large model âœ… = BEST â­

---

## ğŸ”Œ API Model Options

### Default: qwen-flash âœ…

The code now uses **qwen-flash** by default (faster and cheaper than qwen-plus).

### Available Models

| Model | Speed | Cost | Use Case |
|-------|-------|------|----------|
| **qwen-flash** | âš¡ Fastest | ğŸ’° Cheapest | Default (recommended) |
| qwen-plus | ğŸ¢ Medium | ğŸ’°ğŸ’° Medium | More capable |
| qwen-turbo | âš¡ Fast | ğŸ’° Cheap | Alternative |

### Usage Examples

**Hybrid Mode (Cache + API)**:
```bash
# Default: qwen-flash
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache
```

**API-Only Mode (No Cache)**:
```bash
# qwen-flash (fastest, cheapest)
python run_gsm8k_cache_API.py --llm_name qwen-flash

# qwen-plus (more capable)
python run_gsm8k_cache_API.py --llm_name qwen-plus

# qwen-turbo (alternative)
python run_gsm8k_cache_API.py --llm_name qwen-turbo
```

---

## ğŸ® Generation Modes

### Three Ways to Generate Text

#### 1. API_HINT (Default)

**Method**: `generate_text_batch_api`

```bash
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache --generation_mode api_hint
```

**Flow**: Cache â†’ Text hint â†’ API

**Pros**: Simple | **Cons**: No real cache usage

#### 2. HYBRID (Best Quality) â­

**Method**: `generate_text_batch_hybrid`

```bash
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache --generation_mode hybrid
```

**Flow**: Cache â†’ Local model (uses cache) â†’ API refines

**Pros**: Real cache + API quality | **Cons**: Slower

#### 3. LOCAL (Pure Local)

**Method**: `generate_text_batch`

```bash
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache --generation_mode local
```

**Flow**: Cache â†’ Local model (uses cache) â†’ Output

**Pros**: Real cache, no API cost | **Cons**: Lower quality

### Comparison

| Mode | Cache Usage | Quality | Speed | API Cost |
|------|-------------|---------|-------|----------|
| **api_hint** | Text hint | Good | Medium | Yes |
| **hybrid** | Real tensors | **Best** â­ | Slow | Yes |
| **local** | Real tensors | OK | Fast | No |

---

## ğŸ› Bug Fixes

### 1. Missing `get_edge_weight` Method
**Error:** `'GCN' object has no attribute 'get_edge_weight'`

**Location:** `GDesigner/gnn/gcn.py`

**Fix:** Added the missing method to the GCN class:
```python
def get_edge_weight(self, src_id, dst_id):
    """Get edge weight between two nodes (returns 1.0 as default)"""
    return 1.0
```

### 2. Gradient Flow Issue
**Error:** `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn`

**Root Cause:** The `spatial_logits` tensor was being reassigned in `arun()` method, breaking the gradient connection to the GCN parameters.

**Location:** `GDesigner/graph/graph.py` (line ~327)

**Fix:** Changed from:
```python
self.spatial_logits = min_max_norm(torch.flatten(self.spatial_logits))
```

To:
```python
spatial_logits_matrix = logits @ logits.t()
self.spatial_logits = torch.nn.Parameter(
    min_max_norm(torch.flatten(spatial_logits_matrix)),
    requires_grad=self.optimized_spatial
)
```

This ensures the tensor maintains gradient tracking and is properly registered as a Parameter.

### 3. Training Configuration
**Location:** `experiments/run_gsm8k_cache_API.py`

**Changes:**
- Set `optimized_spatial` default to `True` (required for cache training)
- Simplified backprop condition to only check `args.use_cache`

### Key Points

1. **Gradient Flow:** The GCN â†’ MLP â†’ spatial_logits â†’ log_probs chain now properly maintains gradients
2. **Cache Training:** When `--use_cache` is enabled, both GCN and CacheFuser parameters are optimized
3. **Edge Weights:** The `get_edge_weight` method provides default weights (can be extended for learned edge weights)
