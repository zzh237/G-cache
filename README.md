# G-cache: Graph-Guided KV-Cache Communication for Multi-Agent LLMs

Combines **GDesigner's** graph topology with **LatentMAS's** KV-cache generation.

---

## ğŸš€ Quick Start

```bash
cd experiments

# Hybrid mode (RECOMMENDED) - small GPU + free API
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache
```

---

## ğŸ“ What Gets Updated During Training?

### Critical Insight: LatentMAS vs G-cache

**LatentMAS (original):**
- âŒ **NO training** - Inference only!
- âŒ No optimizer, no loss.backward()
- âŒ All models frozen
- Just runs multi-agent inference with cache

**G-cache (this project):**
- âœ… **HAS training** - Learns graph structure!
- âœ… Optimizer updates GCN + CacheFuser
- âœ… Learns from task performance

### Models in G-cache

| Model | Trainable? | Updated? | Purpose | LatentMAS Has This? |
|-------|-----------|----------|---------|---------------------|
| **1. GCN** | âœ… Yes | âœ… Yes | Graph edge weights | âŒ No (NEW in G-cache) |
| **2. CacheFuser** | âœ… Yes | âœ… Yes | Cache fusion weights | âŒ No (NEW in G-cache) |
| **3. Small Local Model** | âŒ Frozen | âŒ No | Cache generation | âœ… Yes (frozen in both) |
| **4. API Model** | âŒ External | âŒ No | Text generation | âœ… Yes (frozen in both) |

### Training Code (G-cache ONLY)

```python
# File: experiments/run_gsm8k_cache_API.py

# Step 1: Setup optimizer with trainable components (NEW in G-cache)
params = list(graph.gcn.parameters())  # â† GCN weights (NEW)
if args.use_cache:
    params += list(graph.cache_fuser.parameters())  # â† CacheFuser weights (NEW)
optimizer = torch.optim.Adam(params, lr=0.1)

# Step 2: Forward pass
answer, log_prob = await graph.arun(question)

# Step 3: Compute loss (NEW in G-cache)
is_correct = (answer == gold_answer)
utility = float(is_correct)
loss = -log_prob * utility

# Step 4: Backprop (NEW in G-cache)
optimizer.zero_grad()
loss.backward()  # Updates:
                 # âœ… GCN.edge_weights (NEW)
                 # âœ… CacheFuser.layer_gates (NEW)
                 # âœ… CacheFuser.fusion_weights (NEW)
                 # âŒ Small local model (frozen - same as LatentMAS)
                 # âŒ API model (external - same as LatentMAS)
optimizer.step()
```

### What LatentMAS Does (Inference Only)

```python
# File: LatentMAS/run.py

# NO optimizer!
# NO loss!
# NO training!

# Just inference:
for item in dataset:
    result = method.run_batch([item])  # Generate answer
    is_correct = (result['prediction'] == item['gold'])
    # That's it - no backprop!
```

### Comparison

| Aspect | LatentMAS | G-cache |
|--------|-----------|---------|
| **Training** | âŒ No | âœ… Yes |
| **Optimizer** | âŒ No | âœ… Yes (Adam) |
| **Loss** | âŒ No | âœ… Yes (-log_prob * utility) |
| **Backprop** | âŒ No | âœ… Yes |
| **Trainable params** | 0 | GCN + CacheFuser |
| **Cache generation** | âœ… Yes (frozen model) | âœ… Yes (frozen model) |
| **Graph structure** | Fixed (sequential/hierarchical) | âœ… Learned |
| **Cache fusion** | Fixed (concatenation) | âœ… Learned |

### Guarantee: No Missing Updates

**LatentMAS updates**: 0 models (inference only)

**G-cache updates**: 2 models
1. âœ… GCN - Added to optimizer (line 108)
2. âœ… CacheFuser - Added to optimizer (line 110)

**Verification:**
```python
# Check what's in optimizer
params = list(graph.gcn.parameters())  # â† All GCN params
params += list(graph.cache_fuser.parameters())  # â† All CacheFuser params
optimizer = torch.optim.Adam(params, lr=0.1)

# These are ALL trainable parameters in the system!
# Small model and API are explicitly frozen/external
```

**Guarantee**: âœ… We update everything that should be updated!

---

## ğŸ”— How Graph Functions Connect with Cache Functions

### The Connection Chain

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. CacheGraph (cache_graph.py)                                 â”‚
â”‚    - Manages cache storage: node_caches = {}                   â”‚
â”‚    - Provides: get_fused_cache(), store_node_cache()           â”‚
â”‚    - Trainable: CacheFuser âœ…                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ passes graph reference
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. MathSolverCache (math_solver_cache.py)                      â”‚
â”‚    - Receives: self.graph = graph                              â”‚
â”‚    - Calls: graph.get_fused_cache(self)                        â”‚
â”‚    - Calls: graph.store_node_cache(self.id, cache)             â”‚
â”‚    - Trainable: None                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ calls LLM
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. HybridCacheLLM (llm_cache_hybrid.py)                        â”‚
â”‚    - Receives: past_key_values from graph                      â”‚
â”‚    - Calls: hybrid_model.generate_latent_batch()               â”‚
â”‚    - Returns: (text, kv_cache) to agent                        â”‚
â”‚    - Trainable: None                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ calls LatentMAS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. HybridCacheModel (hybrid_cache_model.py)                    â”‚
â”‚    - EXACT LatentMAS implementation                            â”‚
â”‚    - generate_latent_batch(past_key_values=fused_cache)        â”‚
â”‚    - Returns: new_cache                                        â”‚
â”‚    - Trainable: None (frozen) âŒ                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Connections Summary

| Connection | From | To | Data | Function |
|------------|------|----|----|----------|
| 1 | CacheGraph | Agent | `graph` reference | `node.graph = self` |
| 2 | Agent | CacheGraph | `self` | `graph.get_fused_cache(self)` |
| 3 | CacheGraph | Agent | `fused_cache` | Returns fused cache |
| 4 | Agent | LLM | `past_key_values` | `llm.agen_with_cache(..., past_key_values)` |
| 5 | LLM | LatentMAS | `past_key_values` | `generate_latent_batch(..., past_key_values)` |
| 6 | LatentMAS | LLM | `new_cache` | Returns cache |
| 7 | LLM | Agent | `(text, cache)` | Returns tuple |
| 8 | Agent | CacheGraph | `cache` | `graph.store_node_cache(id, cache)` |

---

## ğŸ¯ Four Modes

### Mode 1: Pure Local with Real Cache (BEST for Research) âœ…

```bash
cd experiments
export CUDA_VISIBLE_DEVICES=1,2  # Use 2 GPUs for 7B model
python run_gsm8k_cache.py --llm_name Qwen/Qwen3-8B --use_cache --device cuda
```

**What it does**:
- âœ… **Real KV-cache** from local model (LatentMAS)
- âœ… **Alignment matrix** enabled by default
- âœ… Trains GCN + CacheFuser
- âœ… No API needed (fully local)
- âœ… Uses HuggingFace transformers (no vLLM)

**GPU**: ~16GB for 8B model (or ~4GB for 1.7B model)

**Model options**:
- `Qwen/Qwen3-1.7B` - Small, ~4GB GPU
- `Qwen/Qwen3-4B` - Medium, ~8GB GPU  
- `Qwen/Qwen3-8B` - Large, ~16GB GPU (recommended)

**Use this for**: Real cache experiments, full control

---

### Mode 2: Hybrid (Small GPU + API) âœ…

```bash
cd experiments
export CUDA_VISIBLE_DEVICES=1  # Only need 1 GPU
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache
```

**What it does**:
- âœ… Small local model (1.5B) generates **real KV-cache**
- âœ… API (qwen-flash) generates final text (faster & cheaper than qwen-plus)
- âœ… **Alignment matrix** enabled
- âš ï¸ Requires API key

**GPU**: ~4GB only

**API models available**: `qwen-flash` (default), `qwen-plus`, `qwen-turbo`

**Use this for**: When you have API access

---

### Mode 3: API Baseline (No Cache)

```bash
cd experiments
python run_gsm8k_cache_API.py --llm_name qwen-flash  # or qwen-plus, qwen-turbo
```

**What it does**:
- âŒ No real cache (text-based only)
- Uses API for everything
- Baseline for comparison

**GPU**: 0GB

---

### Mode 4: Local Cache (Deprecated)

```bash
cd experiments
export CUDA_VISIBLE_DEVICES=1,2
python run_gsm8k_cache_API.py --llm_name local_cache --use_cache
```

**Note**: Use Mode 1 (`run_gsm8k_cache.py`) instead - it's the same but better tested

---

## ğŸ”§ Setup

```bash
# 1. Install dependencies
pip install torch transformers openai python-dotenv

# 2. Set API key in .env file (already done!)
# Just edit .env:
# API_KEY=your_dashscope_api_key

# 3. Run (no export needed!)
cd experiments
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache
```

**Note**: The code automatically loads `API_KEY` from `.env` - no need to export anything!

---

## ğŸ”¬ LatentMAS Alignment Matrix (Training-Free)

### What is it?

LatentMAS uses a **projection matrix** `W_a` to align hidden states back to valid input embeddings:

```
e = h * W_a, where W_a â‰ˆ W_out^(-1) * W_in
```

**Problem**: Hidden states from last layer have different distribution than input embeddings

**Solution**: Linear transformation that maps output space â†’ input space (training-free!)

### Implementation in LatentMAS

**File**: `LatentMAS/models.py`

```python
class ModelWrapper:
    def _build_latent_realign_matrix(self, model, device, args):
        """Compute W_a = (W_out^T * W_out)^(-1) * W_out^T * W_in"""
        input_weight = model.get_input_embeddings().weight   # W_in
        output_weight = model.get_output_embeddings().weight # W_out
        
        # Solve: W_out * W_a = W_in
        gram = torch.matmul(output_weight.T, output_weight)  # W_out^T * W_out
        reg = 1e-5 * torch.eye(gram.shape[0])                # Regularization
        gram = gram + reg
        rhs = torch.matmul(output_weight.T, input_weight)    # W_out^T * W_in
        realign_matrix = torch.linalg.solve(gram, rhs)       # W_a
        
        target_norm = input_weight.norm(dim=1).mean()        # For normalization
        return realign_matrix, target_norm
    
    def _apply_latent_realignment(self, hidden, model):
        """Apply: e = normalize(h * W_a)"""
        matrix, target_norm = self._ensure_latent_realign_matrix(model, hidden.device)
        aligned = torch.matmul(hidden.float(), matrix)       # h * W_a
        
        # Normalize to match input embedding norms
        aligned_norm = aligned.norm(dim=-1, keepdim=True).clamp_min(1e-6)
        aligned = aligned * (target_norm / aligned_norm)
        return aligned.to(hidden.dtype)
    
    def generate_latent_batch(self, ...):
        for step in range(latent_steps):
            # KEY: Apply alignment before feeding back
            latent_vec = self._apply_latent_realignment(last_hidden, self.model)
            latent_embed = latent_vec.unsqueeze(1)
            
            outputs = self.model(
                inputs_embeds=latent_embed,  # â† Aligned embedding
                ...
            )
```

**Usage**: `python run.py --latent_space_realign` (optional flag)

### Does G-cache Have This?

**Answer**: âœ… **YES! Now implemented!**

**File**: `G-cache/hybrid_cache_model.py`

```python
class HybridCacheModel:
    def __init__(self, ..., use_alignment: bool = True):
        # Build alignment matrix (training-free, computed once)
        if self.use_alignment:
            self._build_alignment_matrix()
    
    def _build_alignment_matrix(self):
        """Build W_a â‰ˆ W_out^(-1) * W_in"""
        W_in = self.cache_model.get_input_embeddings().weight
        W_out = self.cache_model.get_output_embeddings().weight
        gram = torch.matmul(W_out.T, W_out) + 1e-5 * I
        rhs = torch.matmul(W_out.T, W_in)
        self._alignment_matrix = torch.linalg.solve(gram, rhs)
    
    def _apply_alignment(self, hidden):
        """Apply: e = normalize(h * W_a)"""
        aligned = torch.matmul(hidden, self._alignment_matrix)
        return aligned * (target_norm / ||aligned||)
    
    def generate_latent_batch(self, ...):
        for step in range(latent_steps):
            latent_vec = self._apply_alignment(last_hidden)  # âœ… Now aligned!
            latent_embed = latent_vec.unsqueeze(1)
```

**Usage**: Enabled by default! Disable with `use_alignment=False`

### Comparison

| Aspect | LatentMAS | G-cache |
|--------|-----------|---------||
| **Alignment matrix** | âœ… Optional (`--latent_space_realign`) | âœ… **Enabled by default!** |
| **Matrix computation** | `W_a â‰ˆ W_out^(-1) * W_in` | âœ… Same |
| **Normalization** | âœ… Match input embedding norms | âœ… Same |
| **Training-free** | âœ… Yes (computed once) | âœ… Yes (computed once) |
| **Implementation** | 3 methods in ModelWrapper | âœ… 2 methods in HybridCacheModel |

### Benefits in G-cache

**âœ… Enabled by default**:
- Better cache quality (aligned embeddings)
- Training-free (computed once at init)
- Minimal overhead (one matrix multiply per latent step)

**Disable if needed**:
```python
model = HybridCacheModel(use_alignment=False)  # Disable alignment
```

---

## ğŸ¯ Summary

**What**: Graph manages cache flow, LatentMAS generates cache

**LatentMAS**: Inference only (no training) + optional alignment matrix

**G-cache**: Trains GCN + CacheFuser (learns graph structure + fusion)

**Frozen**: Small local model + API model (same as LatentMAS)

**Alignment**: Both have it! (LatentMAS optional, G-cache default)

**Run (Pure Local - RECOMMENDED)**:
```bash
cd experiments
export CUDA_VISIBLE_DEVICES=1,2
python run_gsm8k_cache.py --llm_name Qwen/Qwen3-1.7B --use_cache --device cuda
```

**That's it!** ğŸ‰

---

## ğŸ”„ Cache Flow Explained

### What Are Agent IDs?

Agent IDs like `3KsM`, `QXan`, `5Hp5` are randomly generated 4-character identifiers for each agent node in the graph.

### Agent Graph Structure

```
     3KsM (MathSolver) â†’ cache: 2377 tokens
     QXan (MathSolver) â†’ cache: 474 tokens  
     5Hp5 (MathSolver) â†’ cache: 181 tokens
       â†“ â†“ â†“ (all feed into)
     5xcm (Decision Maker)
       â†“ receives: 3KsM's cache (first one)
       â†“ generates: final answer
```

### What Comes From Cache vs LLM?

| Component | What It Does | Visible? | Example |
|-----------|-------------|----------|----------|
| **Cache (GPU)** | Stores reasoning as tensors | âŒ No | `torch.Size([1, 2, 2377, 128])` |
| **API (qwen-flash)** | Generates text responses | âœ… Yes | "Let's solve step by step..." |
| **Cache Fusion** | Combines multiple caches | âŒ No | Internal process |
| **Graph Structure** | Routes cache between agents | âœ… Yes | Shown in logs |

**Key Insight**: Cache = "compressed thoughts" that agents pass to each other. API converts these thoughts into readable text.

### âš ï¸ Important: How Cache is Used

**Cache is NOT directly used by API!** Here's the actual flow:

1. **Small local model (GPU)** generates KV-cache tensors
2. **Cache is converted to text context**: `"[Using 28 layers of cached reasoning from previous agents, 2377 tokens]"`
3. **This text is prepended to the API prompt**
4. **API (qwen-flash) generates response** based on the text context (NOT the raw cache)

**Why?** APIs don't support KV-cache input - they only accept text. The cache serves as a "hint" to the API about what previous agents computed.

### Cache Flow Example

**Without Cache (Cold Start)**:
```
Agent 3KsM:
  ğŸ†• No input cache
  ğŸ§  Generates from scratch
  ğŸ“ Output: "Let's solve... Total = $64"
  ğŸ’¾ Stores cache: 2377 tokens
```

**With Cache (Warm Start)**:
```
Agent 5xcm:
  ğŸ”— Receives cache from 3KsM (2377 tokens)
  ğŸ§  Continues from 3KsM's reasoning
  ğŸ“ Output: "After reviewing... The answer is 64"
  ğŸ’¾ Stores cache: 3724 tokens (grew from 2377)
```

### Sequence Length Mismatch

**Problem**: Can't fuse caches of different lengths (2377 â‰  474 â‰  181)

**Solution (LatentMAS-proven)**: Use first cache only
- Takes 3KsM's cache (2377 tokens)
- Ignores QXan and 5Hp5 caches
- Sequential passing (proven by LatentMAS paper)

### Debug Markers

Look for these in output:
```
ğŸ”— [CACHE] Using input cache with 28 layers     â† Cache is being used
ğŸ†• [CACHE] No input cache - generating from scratch  â† No cache (first agent)
âœ… [CACHE] Generated cache with 28 layers, seq_len=2377  â† Cache created
ğŸ¤– [API] Calling qwen-flash to generate text...  â† API generates text
ğŸ“ [API] Generated 1234 characters of text       â† Text output
```

### Cache Structure Details

**What is `past_key_values`?**

Type: Tuple of tuples (NOT text, NOT string)

```python
past_key_values = (
    (key_tensor_0, value_tensor_0),    # Layer 0: [1, 2, 2377, 128]
    (key_tensor_1, value_tensor_1),    # Layer 1: [1, 2, 2377, 128]
    # ... 26 more layers ...
    (key_tensor_27, value_tensor_27)   # Layer 27: [1, 2, 2377, 128]
)
```

**Total**: 28 layers Ã— 2 tensors (key + value) = 56 tensors

**Tensor shape**: `[batch=1, heads=2, seq_len=2377, hidden_dim=128]`
- Data type: `torch.bfloat16` (16-bit floating point)
- Device: `cuda:0` (GPU memory)

**Why Can't API Use This?**

API endpoints only accept TEXT, not binary tensors.

**Solution**: Convert to text hint
```python
# Cache (tensor): 56 tensors Ã— millions of numbers
past_key_values = ((tensor1, tensor2), ...)

# Converted to text:
cache_info = "[Using 28 layers of cached reasoning, 2377 tokens]"

# Sent to API:
prompt = cache_info + "\n\n" + original_prompt
```

**Analogy**: Cache = full book (tensor data), API receives = "This person read a 300-page book" (text hint)

### Two Ways to Generate Text

**Method 1: Local Model with Cache Tensors (REAL cache usage)**
```python
# Function: generate_text_batch()
# Uses: Local GPU model (Qwen2.5-1.5B)
# Cache: Passed DIRECTLY as tensors to model.generate()

outputs = model.generate(
    input_ids=input_ids,
    past_key_values=cache_tensors,  # â† REAL cache usage!
    ...
)
```

**Method 2: API with Text Hint (Simulated cache)**
```python
# Function: generate_text_batch_api()
# Uses: API (qwen-flash)
# Cache: Converted to text hint, prepended to prompt

prompt = "[Using 28 layers of cache, 2377 tokens]\n\n" + original_prompt
response = api.chat.completions.create(
    messages=[{"content": prompt}],  # â† Text hint only
    ...
)
```

**Comparison**:

| Aspect | Local (Real Cache) | API (Text Hint) |
|--------|-------------------|------------------|
| **Cache Format** | Tensor objects | Text string |
| **Cache Usage** | Direct (model.generate) | Indirect (text context) |
| **GPU Required** | Yes (~4GB) | No |
| **Speed** | Faster (cache reuse) | Slower (no cache reuse) |
| **Quality** | Good (small model) | Better (large API model) |
| **Cost** | GPU cost | API cost |
| **Best For** | Pure local | API only | **Production** â­ |

**Key Difference**:
- **Local**: `model.generate(past_key_values=cache_tensors)` â† Tensors used directly
- **API**: `api.create(messages=[text_hint])` â† Only text hint sent

**Why API Can't Use Cache Tensors**:
- API endpoints only accept JSON/text
- Can't send binary tensor data over HTTP
- Solution: Convert cache metadata to text hint

**Method 3: TRUE Hybrid (BEST - combines both!)**
```python
# Function: generate_text_batch_hybrid()
# Step 1: Local model uses cache tensors (fast)
# Step 2: API refines output (high quality)

local_text, new_cache = model.generate_text_batch(
    input_ids, past_key_values=cache_tensors  # â† Real cache!
)
api_text, _ = await api.generate_text_batch_api(
    messages_with_local_context  # â† API refines
)
return api_text, new_cache  # Best of both!
```

**Trade-off**:
- Local only: Real cache âœ… but small model âŒ
- API only: Large model âœ… but no cache âŒ
- **Hybrid**: Real cache âœ… + Large model âœ… = BEST â­

---

## ğŸ”Œ API Model Options

### Default: qwen-flash âœ…

The code now uses **qwen-flash** by default (faster and cheaper than qwen-plus).

### Available Models

| Model | Speed | Cost | Use Case |
|-------|-------|------|----------|
| **qwen-flash** | âš¡ Fastest | ğŸ’° Cheapest | Default (recommended) |
| qwen-plus | ğŸ¢ Medium | ğŸ’°ğŸ’° Medium | More capable |
| qwen-turbo | âš¡ Fast | ğŸ’° Cheap | Alternative |

### Usage Examples

**Hybrid Mode (Cache + API)**:
```bash
# Default: qwen-flash
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache
```

**API-Only Mode (No Cache)**:
```bash
# qwen-flash (fastest, cheapest)
python run_gsm8k_cache_API.py --llm_name qwen-flash

# qwen-plus (more capable)
python run_gsm8k_cache_API.py --llm_name qwen-plus

# qwen-turbo (alternative)
python run_gsm8k_cache_API.py --llm_name qwen-turbo
```

---

## ğŸ® Generation Modes

### Three Ways to Generate Text

#### 1. API_HINT (Default)

**Method**: `generate_text_batch_api`

```bash
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache --generation_mode api_hint
```

**Flow**: Cache â†’ Text hint â†’ API

**Pros**: Simple | **Cons**: No real cache usage

#### 2. HYBRID (Best Quality) â­

**Method**: `generate_text_batch_hybrid`

```bash
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache --generation_mode hybrid
```

**Flow**: Cache â†’ Local model (uses cache) â†’ API refines

**Pros**: Real cache + API quality | **Cons**: Slower

#### 3. LOCAL (Pure Local)

**Method**: `generate_text_batch`

```bash
python run_gsm8k_cache_API.py --llm_name hybrid_cache --use_cache --generation_mode local
```

**Flow**: Cache â†’ Local model (uses cache) â†’ Output

**Pros**: Real cache, no API cost | **Cons**: Lower quality

### Comparison

| Mode | Cache Usage | Quality | Speed | API Cost |
|------|-------------|---------|-------|----------|
| **api_hint** | Text hint | Good | Medium | Yes |
| **hybrid** | Real tensors | **Best** â­ | Slow | Yes |
| **local** | Real tensors | OK | Fast | No |

---

## ğŸ› Bug Fixes

### 1. Missing `get_edge_weight` Method
**Error:** `'GCN' object has no attribute 'get_edge_weight'`

**Location:** `GDesigner/gnn/gcn.py`

**Fix:** Added the missing method to the GCN class:
```python
def get_edge_weight(self, src_id, dst_id):
    """Get edge weight between two nodes (returns 1.0 as default)"""
    return 1.0
```

### 2. Gradient Flow Issue
**Error:** `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn`

**Root Cause:** The `spatial_logits` tensor was being reassigned in `arun()` method, breaking the gradient connection to the GCN parameters.

**Location:** `GDesigner/graph/graph.py` (line ~327)

**Fix:** Changed from:
```python
self.spatial_logits = min_max_norm(torch.flatten(self.spatial_logits))
```

To:
```python
spatial_logits_matrix = logits @ logits.t()
self.spatial_logits = torch.nn.Parameter(
    min_max_norm(torch.flatten(spatial_logits_matrix)),
    requires_grad=self.optimized_spatial
)
```

This ensures the tensor maintains gradient tracking and is properly registered as a Parameter.

### 3. Training Configuration
**Location:** `experiments/run_gsm8k_cache_API.py`

**Changes:**
- Set `optimized_spatial` default to `True` (required for cache training)
- Simplified backprop condition to only check `args.use_cache`

### Key Points

1. **Gradient Flow:** The GCN â†’ MLP â†’ spatial_logits â†’ log_probs chain now properly maintains gradients
2. **Cache Training:** When `--use_cache` is enabled, both GCN and CacheFuser parameters are optimized
3. **Edge Weights:** The `get_edge_weight` method provides default weights (can be extended for learned edge weights)


The API call flow 
FinalRefer â†’ HybridCacheLLM.agen() â†’ hybrid_model.generate_text_batch_api() 
â†’ AsyncOpenAI.chat.completions.create(model="qwen-flash", max_tokens=4096)
â†’ Your .env BASE_URL + API_KEY


Step-by-Step Call Chain:
1. run_gpqa_cache_API.py (line ~200)
   answer_log_probs.append(asyncio.create_task(graph.arun(input_dict, args.num_rounds)))
   â†“

2. graph.arun() (graph.py line 374)
   - Constructs graph connections
   - Executes nodes in topological order
   - Line 402: await self.nodes[current_node_id].async_execute(input)
   â†“

3. node.async_execute() (node.py line 157)
   - Gets spatial_info and temporal_info
   - Line 162: self._async_execute(input, spatial_info, temporal_info)
   â†“

4. MathSolverCache._async_execute() (math_solver_cache.py line 101)
   - Your STEP 3-11 logs
   - Line 175: return response
   â†“

5. Back to node.async_execute() (node.py line 168)
   - Stores result in self.outputs
   - Returns self.outputs
   â†“

6. Back to graph.arun() (graph.py line 418-420)
   - After all nodes execute, calls decision_node
   - Returns: final_answers, log_probs
   â†“

7. Back to run_gpqa_cache_API.py (line ~210)
   raw_results = await asyncio.gather(*answer_log_probs)
   raw_answers, log_probs = zip(*raw_results)




# CacheDesigner Workflow Tracking Guide

## Complete Execution Flow with Step Numbers

### High-Level Overview
```
User Task â†’ Graph Execution â†’ Multi-Agent Processing â†’ Cache Communication â†’ Final Response
```

### Detailed Step-by-Step Flow

#### **STEP 1-2: Graph Initialization**
- **Location:** `cache_graph.py:169`
- **Print:** `ğŸš€ [STEP 1] CacheGraph.arun() - Starting graph execution`
- **What happens:** 
  - Clears node caches
  - Sets graph reference on all nodes
  - Calls parent Graph.arun()

#### **STEP 3: Agent Execution Starts**
- **Location:** `math_solver_cache.py:113`
- **Print:** `ğŸ¯ [STEP 3] MathSolverCache._async_execute() - Executing node {node_id}`
- **What happens:** Agent begins processing its task

#### **STEP 4: Cache Retrieval**
- **Location:** `cache_graph.py:127`
- **Print:** `ğŸ”„ [STEP 4] CacheGraph.get_fused_cache() - Getting fused cache for node {node_id}`
- **What happens:** 
  - Checks for spatial predecessors
  - Fuses caches from predecessor nodes
  - Returns None for root nodes (no predecessors)

#### **STEP 5: Cache Status**
- **Location:** `math_solver_cache.py:118`
- **Print:** `ğŸ“¥ [STEP 5] MathSolverCache - Received fused cache: {True/False}`
- **What happens:** Agent receives (or doesn't receive) cache from predecessors

#### **STEP 5a: Prompt Building**
- **Location:** `math_solver_cache.py:121`
- **Print:** `ğŸ“ [STEP 5a] MathSolverCache._process_inputs() - Building prompt with context`
- **What happens:**
  - Combines task + spatial context + temporal context
  - Builds system and user prompts
  - Creates messages array

#### **STEP 6: LLM Call**
- **Location:** `math_solver_cache.py:126`
- **Print:** `ğŸ§  [STEP 6] MathSolverCache - Calling llm.agen_with_cache() (Goal: Generate reasoning cache + text response)`
- **What happens:** Agent calls LLM to generate response with cache

#### **STEP 7: Cache Generation Starts**
- **Location:** `llm_cache_hybrid.py:81`
- **Print:** `ğŸ“¦ [STEP 7] HybridCacheLLM.agen_with_cache() - Starting cache generation`
- **What happens:** LLM wrapper begins processing

#### **STEP 7a: Message Conversion**
- **Location:** `llm_cache_hybrid.py:84`
- **Print:** `ğŸ’¬ [STEP 7a] Converting messages to text prompt...`
- **What happens:** Converts chat messages to text format

#### **STEP 7b: Tokenization**
- **Location:** `llm_cache_hybrid.py:88`
- **Print:** `ğŸ”¤ [STEP 7b] Tokenizing prompt...`
- **What happens:**
  - Clears CUDA cache
  - Tokenizes prompt
  - Validates token IDs
  - Moves tensors to device

#### **STEP 8: Latent Cache Generation**
- **Location:** `llm_cache_hybrid.py:135`
- **Print:** `ğŸ”— [STEP 8] HybridCacheLLM - Calling hybrid_model.generate_latent_batch()`
- **What happens:** Calls model to generate KV-cache

#### **STEP 8a: Cache Model Forward Pass**
- **Location:** `hybrid_cache_model.py:102`
- **Print:** `ğŸ§  [STEP 8a] HybridCacheModel.generate_latent_batch() - Generating cache using small local model`
- **What happens:**
  - Forward pass through local model
  - 10 latent reasoning steps
  - Builds KV-cache (no text yet!)

#### **STEP 9: Text Generation**
- **Location:** `llm_cache_hybrid.py:145`
- **Print:** `ğŸ“ [STEP 9] HybridCacheLLM - Generating text with mode: {mode}`
- **What happens:** Chooses generation mode (hybrid/local/api_hint)

#### **STEP 9a: Local Model Generation** (if mode=local or hybrid)
- **Location:** `hybrid_cache_model.py:54`
- **Print:** `ğŸ¯ [STEP 9a] HybridCacheModel.generate_text_batch() - Generating text using cache TENSORS directly`
- **What happens:**
  - Uses KV-cache from STEP 8
  - Calls HuggingFace model.generate()
  - Generates text tokens autoregressively

#### **STEP 9b: API Generation** (if mode=api_hint)
- **Location:** `hybrid_cache_model.py:130`
- **Print:** `ğŸ”„ [STEP 9b] HybridCacheModel.generate_text_batch_api() - Converting cache to text context`
- **What happens:**
  - Converts cache to text hint
  - Calls API (qwen-plus/qwen-flash)
  - Returns API response

#### **STEP 10: Cache Storage**
- **Location:** `math_solver_cache.py:136`
- **Print:** `ğŸ’¾ [STEP 10] MathSolverCache - Calling store_node_cache() for {n} successors`
- **What happens:** Stores generated cache for successor nodes to use

#### **STEP 11: Response Return**
- **Location:** `math_solver_cache.py:141`
- **Print:** `ğŸ¯ [STEP 11] MathSolverCache - Returning final response from node {node_id}`
- **What happens:** Agent returns its final text response

#### **STEP 12: Graph Aggregation**
- **Location:** `run_gsm8k_cache_API.py:175`
- **Print:** `â³ [STEP 12] Waiting for graph.arun() to complete for {n} tasks...`
- **What happens:** Waits for all agents to finish

#### **STEP 13: Results Collection**
- **Location:** `run_gsm8k_cache_API.py:178`
- **Print:** `ğŸ [STEP 13] Graph execution complete - received {n} responses`
- **What happens:** 
  - Collects all agent responses
  - Shows preview of each response
  - **THIS IS WHERE YOUR OUTPUT COMES FROM!**

#### **STEP 14: Metrics Computation**
- **Location:** `run_gsm8k_cache_API.py:183`
- **Print:** `ğŸ“Š [STEP 14] Processing results and computing metrics...`
- **What happens:**
  - Extracts predicted answers
  - Computes accuracy
  - Saves results to JSON

---

## Understanding Your Output

The output you see:
```
The task is: Kylar went to the store...
At the same time, the output of other agents is as follows:
7LdC: Let's solve this step by step...
5qxQ: Let's analyze the problem...
...
################response:After reviewing the analysis...
```

**Comes from STEP 13** - This is the **final aggregated response** from the graph after all agents have completed.

### Where Each Part Comes From:

1. **"The task is:"** - From the prompt template in `_process_inputs()` (STEP 5a)
2. **"output of other agents"** - From spatial context (other agents in same round)
3. **"7LdC:", "5qxQ:", etc.** - Individual agent responses (STEP 11 from each agent)
4. **"################response:"** - Final decision/aggregation (from decision agent or final node)

### Is it from Cache or Direct Model?

The text is generated in **STEP 9**:
- **If mode=local:** Direct from local model using cache tensors (STEP 9a)
- **If mode=api_hint:** From API with cache converted to text hint (STEP 9b)
- **If mode=hybrid:** Local model first, then API refinement

The cache itself (KV-cache tensors) is generated in **STEP 8a** and contains the "reasoning" in latent space, but the actual text you see is generated in **STEP 9**.

---

## Quick Reference

| Step | Component | What You See |
|------|-----------|--------------|
| 1-2 | Graph Init | Graph setup messages |
| 3-6 | Agent Setup | Agent execution start |
| 7-7b | Tokenization | Prompt processing |
| 8-8a | Cache Gen | KV-cache creation (latent reasoning) |
| 9-9b | Text Gen | **Actual text generation** |
| 10-11 | Cache Store | Cache saved for next nodes |
| 12-13 | Aggregation | **Final output collection** |
| 14 | Metrics | Accuracy computation |

**Your output appears at STEP 13** - it's the final aggregated response after all agents complete their processing.


æˆ‘æ¥è¯¦ç»†è§£é‡ŠTransformeræ¨¡å‹ä¸­çš„forward passå’Œgenerationè¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ‰€æœ‰ç»´åº¦ä¿¡æ¯ã€‚

1. Hidden State H(l) çš„è®¡ç®—è¿‡ç¨‹
H(l) âˆˆ R^(1Ã—690Ã—2560) æ˜¯è¿™æ ·å¾—åˆ°çš„ï¼š

å®Œæ•´çš„Transformer Layerè®¡ç®—ï¼š

è¾“å…¥: H(l-1) âˆˆ R^(BÃ—TÃ—D)  (B=1, T=690, D=2560)

æ­¥éª¤1: Self-Attention
------
Q = H(l-1) @ W_Q  â†’  Q âˆˆ R^(1Ã—690Ã—2560)
K = H(l-1) @ W_K  â†’  K âˆˆ R^(1Ã—690Ã—2560)  
V = H(l-1) @ W_V  â†’  V âˆˆ R^(1Ã—690Ã—2560)

æ­¥éª¤2: Multi-Head Attention (å‡è®¾32ä¸ªheads)
------
å°†Q,K,Våˆ†æˆ32ä¸ªheads:
Q_heads âˆˆ R^(1Ã—32Ã—690Ã—80)  (2560/32=80)
K_heads âˆˆ R^(1Ã—32Ã—690Ã—80)
V_heads âˆˆ R^(1Ã—32Ã—690Ã—80)

æ­¥éª¤3: Scaled Dot-Product Attention
------
scores = (Q_heads @ K_heads^T) / sqrt(80)  â†’  scores âˆˆ R^(1Ã—32Ã—690Ã—690)

æ­¥éª¤4: Apply Attention Mask (è¿™é‡Œå›ç­”ä½ çš„maské—®é¢˜)
------
# attention_mask âˆˆ R^(1Ã—690), å€¼ä¸º0æˆ–1
# æ‰©å±•ä¸º R^(1Ã—1Ã—690Ã—690) çš„causal mask
mask = (1 - attention_mask) * (-10000)  # 0å˜0, 1å˜-10000
scores = scores + mask  # Element-wiseåŠ æ³•!

æ­¥éª¤5: Softmax (åœ¨sequenceç»´åº¦ä¸Š)
------
attn_weights = softmax(scores, dim=-1)  # åœ¨æœ€åä¸€ç»´(690)ä¸Šåšsoftmax
# attn_weights âˆˆ R^(1Ã—32Ã—690Ã—690)

æ­¥éª¤6: Apply to Values
------
attn_output = attn_weights @ V_heads  â†’  R^(1Ã—32Ã—690Ã—80)

æ­¥éª¤7: Concat heads
------
attn_output = concat(attn_output)  â†’  R^(1Ã—690Ã—2560)

æ­¥éª¤8: Output projection
------
O = attn_output @ W_O  â†’  O âˆˆ R^(1Ã—690Ã—2560)

æ­¥éª¤9: Add & Norm
------
H_attn = LayerNorm(H(l-1) + O)  â†’  R^(1Ã—690Ã—2560)

æ­¥éª¤10: FFN
------
H(l) = LayerNorm(H_attn + FFN(H_attn))  â†’  R^(1Ã—690Ã—2560)


Copy
æ‰€ä»¥æ˜¯çš„ï¼Œéœ€è¦QKVï¼ æ¯ä¸€å±‚éƒ½è¦è®¡ç®—QKVã€‚

2. Attention Mask çš„ä½œç”¨æœºåˆ¶
# attention_mask ç¤ºä¾‹: [1, 1, 1, 0, 0]  (å‰3ä¸ªtokenæœ‰æ•ˆï¼Œå2ä¸ªpadding)

# æ­¥éª¤1: è½¬æ¢ä¸ºadditive mask
mask = (1 - attention_mask) * (-10000)
# ç»“æœ: [0, 0, 0, -10000, -10000]

# æ­¥éª¤2: æ‰©å±•åˆ°attention scoresç»´åº¦
# scores âˆˆ R^(BÃ—headsÃ—TÃ—T)
# maskæ‰©å±•ä¸º R^(BÃ—1Ã—1Ã—T) ç„¶åbroadcast

# æ­¥éª¤3: Element-wise åŠ æ³• (ä¸æ˜¯ä¹˜æ³•!)
scores = scores + mask
# ä¾‹å¦‚æŸä¸ªä½ç½®çš„scoreæ˜¯5.2ï¼Œå¦‚æœmaskæ˜¯-10000ï¼Œåˆ™å˜æˆ-9994.8

# æ­¥éª¤4: Softmax
attn_weights = softmax(scores, dim=-1)
# exp(-10000) â‰ˆ 0ï¼Œæ‰€ä»¥è¢«maskçš„ä½ç½®æƒé‡â‰ˆ0

å…³é”®ç‚¹ï¼š

Maskæ˜¯ åŠ æ³• ï¼Œä¸æ˜¯ä¹˜æ³•

Softmaxæ˜¯åœ¨ sequence level ï¼ˆæœ€åä¸€ç»´ï¼‰ä¸Šåšçš„

è¢«maskçš„ä½ç½®ç»è¿‡softmaxåæƒé‡â‰ˆ0ï¼Œç›¸å½“äº"knock out"

3. Generationè¿‡ç¨‹è¯¦è§£
Prefillé˜¶æ®µï¼ˆç¬¬ä¸€æ¬¡forwardï¼‰

è¾“å…¥: input_ids âˆˆ R^(1Ã—T)  ä¾‹å¦‚ T=10
past_kv = None

æ­¥éª¤1: Embedding
------
H(0) = Embedding(input_ids)  â†’  H(0) âˆˆ R^(1Ã—10Ã—2560)

æ­¥éª¤2: é€šè¿‡æ‰€æœ‰Transformer layers
------
for layer in layers:
    H(l) = TransformerLayer(H(l-1))  # å¦‚ä¸Šé¢è¯¦ç»†è¿‡ç¨‹
    # åŒæ—¶ç”Ÿæˆå¹¶ä¿å­˜ K(l), V(l) âˆˆ R^(1Ã—32Ã—10Ã—80)

æœ€ç»ˆ: H(L) âˆˆ R^(1Ã—10Ã—2560)  (Læ˜¯æœ€åä¸€å±‚)

æ­¥éª¤3: å–æœ€åä¸€ä¸ªtokençš„hidden state
------
h_last = H(L)[:, -1, :]  â†’  h_last âˆˆ R^(1Ã—2560)

æ­¥éª¤4: ç”Ÿæˆlogits
------
logits = h_last @ W_lm_head  â†’  logits âˆˆ R^(1Ã—vocab_size)

æ­¥éª¤5: é‡‡æ ·next token
------
next_token_id = sample(logits)  â†’  scalar

æ­¥éª¤6: ä¿å­˜KV cache
------
past_kv = [(K(1), V(1)), (K(2), V(2)), ..., (K(L), V(L))]
# æ¯ä¸ª K(l), V(l) âˆˆ R^(1Ã—32Ã—10Ã—80)

Autoregressiveé˜¶æ®µï¼ˆåç»­ç”Ÿæˆï¼‰

è¾“å…¥: next_token_id (scalar)
past_kv = [(K, V) for each layer]  # K,V âˆˆ R^(1Ã—32Ã—T_pastÃ—80)

æ­¥éª¤1: Embedding
------
h_new = Embedding(next_token_id)  â†’  h_new âˆˆ R^(1Ã—1Ã—2560)

æ­¥éª¤2: é€šè¿‡æ‰€æœ‰layers (ä½¿ç”¨KV cache!)
------
for layer_idx, layer in enumerate(layers):
    K_past, V_past = past_kv[layer_idx]  # R^(1Ã—32Ã—T_pastÃ—80)
    
    # åªè®¡ç®—æ–°tokençš„Q,K,V
    Q_new = h_new @ W_Q  â†’  R^(1Ã—32Ã—1Ã—80)
    K_new = h_new @ W_K  â†’  R^(1Ã—32Ã—1Ã—80)
    V_new = h_new @ W_V  â†’  R^(1Ã—32Ã—1Ã—80)
    
    # Concat with past
    K_full = concat([K_past, K_new], dim=2)  â†’  R^(1Ã—32Ã—(T_past+1)Ã—80)
    V_full = concat([V_past, V_new], dim=2)  â†’  R^(1Ã—32Ã—(T_past+1)Ã—80)
    
    # Attention (Qåªæœ‰1ä¸ªtokenï¼Œä½†attend toæ‰€æœ‰past tokens)
    scores = Q_new @ K_full^T  â†’  R^(1Ã—32Ã—1Ã—(T_past+1))
    attn = softmax(scores, dim=-1)
    output = attn @ V_full  â†’  R^(1Ã—32Ã—1Ã—80)
    
    # ç»§ç»­FFNç­‰...
    h_new = TransformerLayer(h_new, K_full, V_full)  â†’  R^(1Ã—1Ã—2560)
    
    # æ›´æ–°cache
    past_kv[layer_idx] = (K_full, V_full)

æ­¥éª¤3: ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
------
h_last = h_new[:, -1, :]  â†’  R^(1Ã—2560)
logits = h_last @ W_lm_head  â†’  R^(1Ã—vocab_size)
next_token = sample(logits)

4. å…³äºä½ çš„ä»£ç ä¸­çš„latent generation
åœ¨ä½ çš„ä»£ç ä¸­ï¼š

# generate_latent_batch() - åªç”Ÿæˆcacheï¼Œä¸ç”Ÿæˆtoken
outputs = self.cache_model(
    inputs_embeds=latent_embed,  # R^(1Ã—1Ã—2560)
    past_key_values=past,
    use_cache=True,
    output_hidden_states=True,
    return_dict=True,
)
# æ³¨æ„: è¿™é‡Œæ²¡æœ‰è°ƒç”¨ .generate()
# æ‰€ä»¥æ²¡æœ‰ outputs.sequences å±æ€§!
# åªæœ‰ outputs.hidden_states å’Œ outputs.past_key_values

ä½ çš„é”™è¯¯åŸå› ï¼š

output_seq_len = outputs.sequences.shape[1]  # âŒ é”™è¯¯!
# CausalLMOutputWithPast æ²¡æœ‰ sequences å±æ€§
# sequences åªåœ¨ .generate() çš„è¿”å›å€¼ä¸­å­˜åœ¨